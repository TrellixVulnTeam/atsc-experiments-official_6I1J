{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_mams-test.ipynb","provenance":[{"file_id":"1-sCaCeUfkST8npRaguw59pW61U5nCOnC","timestamp":1618500028257},{"file_id":"15JlKkhGtbIHPXZBPynwMRKQcExWvqnRT","timestamp":1618499328888},{"file_id":"1jwSjHyEPiHzovPM5fjtYliW8YuJsC5NU","timestamp":1618497973114},{"file_id":"13Uuiig-8uZ_Bkg6NssQEOLujDkx4-yJ0","timestamp":1618494402410},{"file_id":"1HZRcbuLT7dKHVL_c5AXMj8cpXNUQz8Cx","timestamp":1618482437523},{"file_id":"10uitUAMMugkT4GzWH8-Fym8W-jGtkt1-","timestamp":1618481070596},{"file_id":"17vlkZFBPb3QsvbWkBLlDQG_QeqO-JZdm","timestamp":1618473918711},{"file_id":"1AT_0GiUm-x0jiLImp6kBPj2bJliUU7u9","timestamp":1612030190493}],"collapsed_sections":["h_WVE4pSNbd8","FK6Qt7kHWcwu","TA33P7z7pJ26","Yhf5gFid8__f","z8m9Y4Idr9Sa","p4i-GN5Q1KlZ"],"authorship_tag":"ABX9TyPbYwBdZnLMckB/aZVveIhb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TF99dpCJYsr","executionInfo":{"status":"ok","timestamp":1624894827796,"user_tz":-120,"elapsed":14885,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"34bb223c-b748-412d-c047-d60276850875"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"source":["TODO: adjust the following paths"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#data_path = \".../Data/MAMS/mams_test.xml\"\n","#output_path = \".../Data/Final/MAMS/\"\n","#aspect_path = \".../preprocessing/aspect_positions/\""]},{"cell_type":"markdown","metadata":{"id":"D3r8d-AzWGcf"},"source":["# Data Import"]},{"cell_type":"code","metadata":{"id":"1HsS1-jUzDZp","executionInfo":{"status":"ok","timestamp":1624894827801,"user_tz":-120,"elapsed":18,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["import pandas as pd\n","import xml.etree.ElementTree as ET"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"12vjxMVvzMR2","executionInfo":{"status":"ok","timestamp":1624894828648,"user_tz":-120,"elapsed":862,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["parsedXML = ET.parse(data_path)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5n6BloJzkKq7","executionInfo":{"status":"ok","timestamp":1624894828656,"user_tz":-120,"elapsed":16,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["aspect_number = 12"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P07rAjP_lNXG","executionInfo":{"status":"ok","timestamp":1624894832953,"user_tz":-120,"elapsed":4308,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["dfcols = ['id','text']\n","for ii in range(1,aspect_number):\n","    dfcols.append(\"aspect_term_{}\".format(ii))\n","    dfcols.append(\"aspect_polarity_{}\".format(ii))\n","    dfcols.append(\"aspect_from_{}\".format(ii))\n","    dfcols.append(\"aspect_to_{}\".format(ii))          \n","df = pd.DataFrame(columns=dfcols)\n","\n","for sentence in parsedXML.getroot():\n","    id = sentence.attrib.get('id')\n","    text = sentence.find('text').text\n","    line = [id,text]\n","\n","    for asp in sentence.iter('aspectTerm'):\n","        term = asp.attrib.get(\"term\")\n","        pol = asp.attrib.get(\"polarity\")\n","        a_from = asp.attrib.get(\"from\")\n","        a_to = asp.attrib.get(\"to\")\n","\n","        line += [term, pol, a_from, a_to]\n","\n","    if len(line) < len(dfcols):\n","        pads = [None] * (len(dfcols)-len(line))\n","        line += pads\n","    \n","    df = df.append(pd.Series(line, index=dfcols), ignore_index=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nsuk3xIs5UUE","executionInfo":{"status":"ok","timestamp":1624894833341,"user_tz":-120,"elapsed":398,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"539d570d-f41a-447b-eb0e-ff93798be846"},"source":["df"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","      <th>aspect_term_11</th>\n","      <th>aspect_polarity_11</th>\n","      <th>aspect_from_11</th>\n","      <th>aspect_to_11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>None</td>\n","      <td>The food was served promptly but the meal wasn...</td>\n","      <td>food</td>\n","      <td>neutral</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>served</td>\n","      <td>positive</td>\n","      <td>13</td>\n","      <td>19</td>\n","      <td>appetizers</td>\n","      <td>positive</td>\n","      <td>93</td>\n","      <td>103</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>None</td>\n","      <td>When I got home, there was a message on the ma...</td>\n","      <td>owner</td>\n","      <td>neutral</td>\n","      <td>64</td>\n","      <td>69</td>\n","      <td>waitress</td>\n","      <td>negative</td>\n","      <td>88</td>\n","      <td>96</td>\n","      <td>wine</td>\n","      <td>neutral</td>\n","      <td>125</td>\n","      <td>129</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","      <td>The Scene Shun Lee Palace is popular with midt...</td>\n","      <td>Scene</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>lunch</td>\n","      <td>neutral</td>\n","      <td>144</td>\n","      <td>149</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>None</td>\n","      <td>To both our surprise, this inquiry was interpr...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>80</td>\n","      <td>86</td>\n","      <td>pastries</td>\n","      <td>neutral</td>\n","      <td>145</td>\n","      <td>153</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>None</td>\n","      <td>In fact you can find their menu offerings at o...</td>\n","      <td>menu</td>\n","      <td>neutral</td>\n","      <td>27</td>\n","      <td>31</td>\n","      <td>prices</td>\n","      <td>positive</td>\n","      <td>89</td>\n","      <td>95</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>None</td>\n","      <td>The wait staff is slow, full of attitude and f...</td>\n","      <td>wait staff</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>116</td>\n","      <td>122</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>None</td>\n","      <td>Our waiter was inattentive throughout the meal...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>meal</td>\n","      <td>neutral</td>\n","      <td>42</td>\n","      <td>46</td>\n","      <td>dinner</td>\n","      <td>neutral</td>\n","      <td>72</td>\n","      <td>78</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>None</td>\n","      <td>In Short The much-copied look of hardwood floo...</td>\n","      <td>stained glass</td>\n","      <td>positive</td>\n","      <td>61</td>\n","      <td>74</td>\n","      <td>walls</td>\n","      <td>neutral</td>\n","      <td>113</td>\n","      <td>118</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>None</td>\n","      <td>Pair you food with the excellent beers on tap ...</td>\n","      <td>food</td>\n","      <td>neutral</td>\n","      <td>9</td>\n","      <td>13</td>\n","      <td>beers on tap</td>\n","      <td>positive</td>\n","      <td>33</td>\n","      <td>45</td>\n","      <td>priced wine list</td>\n","      <td>positive</td>\n","      <td>60</td>\n","      <td>76</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>None</td>\n","      <td>They were out of Hennessy, Alize, Mojitos, Pin...</td>\n","      <td>Mojitos</td>\n","      <td>neutral</td>\n","      <td>34</td>\n","      <td>41</td>\n","      <td>Pinot</td>\n","      <td>neutral</td>\n","      <td>43</td>\n","      <td>48</td>\n","      <td>drink list</td>\n","      <td>negative</td>\n","      <td>158</td>\n","      <td>168</td>\n","      <td>drinks</td>\n","      <td>negative</td>\n","      <td>238</td>\n","      <td>244</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 46 columns</p>\n","</div>"],"text/plain":["       id  ... aspect_to_11\n","0    None  ...         None\n","1    None  ...         None\n","2    None  ...         None\n","3    None  ...         None\n","4    None  ...         None\n","..    ...  ...          ...\n","495  None  ...         None\n","496  None  ...         None\n","497  None  ...         None\n","498  None  ...         None\n","499  None  ...         None\n","\n","[500 rows x 46 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"863AbS67LrD2"},"source":["# Drop duplicates"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"16yC8XZHoisI","executionInfo":{"status":"ok","timestamp":1624894833342,"user_tz":-120,"elapsed":33,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"0f3bdf9c-19da-4f94-a35e-5d377c68edf8"},"source":["cols_wo_id = df.columns\n","cols_wo_id = cols_wo_id.drop(\"id\")\n","df[df.duplicated(cols_wo_id, keep=False)].sort_values(\"text\")"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","      <th>aspect_term_11</th>\n","      <th>aspect_polarity_11</th>\n","      <th>aspect_from_11</th>\n","      <th>aspect_to_11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [id, text, aspect_term_1, aspect_polarity_1, aspect_from_1, aspect_to_1, aspect_term_2, aspect_polarity_2, aspect_from_2, aspect_to_2, aspect_term_3, aspect_polarity_3, aspect_from_3, aspect_to_3, aspect_term_4, aspect_polarity_4, aspect_from_4, aspect_to_4, aspect_term_5, aspect_polarity_5, aspect_from_5, aspect_to_5, aspect_term_6, aspect_polarity_6, aspect_from_6, aspect_to_6, aspect_term_7, aspect_polarity_7, aspect_from_7, aspect_to_7, aspect_term_8, aspect_polarity_8, aspect_from_8, aspect_to_8, aspect_term_9, aspect_polarity_9, aspect_from_9, aspect_to_9, aspect_term_10, aspect_polarity_10, aspect_from_10, aspect_to_10, aspect_term_11, aspect_polarity_11, aspect_from_11, aspect_to_11]\n","Index: []"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"7VcO-JpTt4ev","executionInfo":{"status":"ok","timestamp":1624894833343,"user_tz":-120,"elapsed":31,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["df.drop_duplicates(cols_wo_id, ignore_index=True,inplace=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42Horu_479nq","executionInfo":{"status":"ok","timestamp":1624894833344,"user_tz":-120,"elapsed":30,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"4f9918aa-c4ae-45f3-eebe-415c987e8cef"},"source":["text_counts = df.text.value_counts()\n","text_counts[:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["The food was great so that made up for the lack of service from the waitress.                                                                                                                                                                                                                                                                                1\n","The Food Consulting Chef Douglas Rodriguez's forte is blending the diversity of traditional Latin cuisine with modern culinary chic: A celery sorbet cools the spicy-citrus bite of lobster; shrimp seviche and crunchy green plantains coat thick, juicy halibut resting on sweet plantain hash; and tart tomato escabeche moistens a chewy skirt steak.    1\n","the service was slow took ten mins to get me a glass of water.                                                                                                                                                                                                                                                                                               1\n","Art and miscellany adorn the black-painted walls; strings of colored lights, disco balls and TVs are hung here and there; and on weekends a DJ fills the dining room with loud dance beats.                                                                                                                                                                  1\n","The concoction was thrown away a mere two blocks from the Bar.                                                                                                                                                                                                                                                                                               1\n","The servers were snobby and got mad at me when I asked if they serve by the slice.                                                                                                                                                                                                                                                                           1\n","The food was barely decent and our server was nowhere to be found.                                                                                                                                                                                                                                                                                           1\n","Most of the food has a vinegary flavor (I think because of the injera).                                                                                                                                                                                                                                                                                      1\n","The quality of the sushi was bad, the way it was cut was bad, and the waiters kept trying to clear off my miso soup even when it wasn't finished.                                                                                                                                                                                                            1\n","Ok I got the edamame and something from the sushi chef for free, but the quality of food is more important to me than a free small dish (maybe that's why the restaurant gives it to attact new customers.                                                                                                                                                   1\n","Name: text, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"k45Zoj6iWMcq"},"source":["# First Descriptive Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"gQkriKTZKlcm"},"source":["## Aspects per sentence"]},{"cell_type":"code","metadata":{"id":"fNv56kkP3OKP","executionInfo":{"status":"ok","timestamp":1624894833345,"user_tz":-120,"elapsed":27,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["pol_cols = [\"aspect_polarity_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ks804oRfuKxd","executionInfo":{"status":"ok","timestamp":1624894833346,"user_tz":-120,"elapsed":28,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"3c88e002-a570-4c96-96f4-bdbc16eb5388"},"source":["prev = 0\n","total_asp = 0\n","for no, col in enumerate(pol_cols):\n","\n","    if col != \"aspect_polarity_1\":\n","        print(\"sentences with exactly \", no, \"aspects:\", prev - sum(df[col].value_counts()))\n","        total_asp += no * (prev - sum(df[col].value_counts()))\n","\n","    if col == \"aspect_polarity_\"+str(aspect_number-1):\n","        print(\"sentences with exactly \", no+1, \"aspects:\", sum(df[col].value_counts()))\n","        total_asp += (no+1) * sum(df[col].value_counts())\n","\n","\n","    prev = sum(df[col].value_counts())\n","\n","print(\"total no of aspects: \", total_asp)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["sentences with exactly  1 aspects: 0\n","sentences with exactly  2 aspects: 264\n","sentences with exactly  3 aspects: 173\n","sentences with exactly  4 aspects: 45\n","sentences with exactly  5 aspects: 10\n","sentences with exactly  6 aspects: 5\n","sentences with exactly  7 aspects: 0\n","sentences with exactly  8 aspects: 1\n","sentences with exactly  9 aspects: 0\n","sentences with exactly  10 aspects: 1\n","sentences with exactly  11 aspects: 1\n","total no of aspects:  1336\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h_WVE4pSNbd8"},"source":["## Sentiment Frequency"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xitO9zfJUBWD","executionInfo":{"status":"ok","timestamp":1624894833347,"user_tz":-120,"elapsed":25,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"8c980fcf-c186-4a09-e318-a4a36ea97e84"},"source":["df_pol = df.loc[:,pol_cols]\n","df_pol_counts = df_pol.apply(pd.Series.value_counts)\n","df_pol_counts.sum(axis=1)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    329.0\n","neutral     607.0\n","positive    400.0\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIhdlntE0lpQ","executionInfo":{"status":"ok","timestamp":1624894833349,"user_tz":-120,"elapsed":25,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"7a45980b-264a-43d5-a4a7-f56a7026f8a0"},"source":["sum(df_pol_counts.sum(axis=1))"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1336.0"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"FK6Qt7kHWcwu"},"source":["## Sentences with more than one aspect"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjRJET_vRnfE","executionInfo":{"status":"ok","timestamp":1624894833349,"user_tz":-120,"elapsed":21,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"1cad773f-c033-4b6f-c0a6-e306019f4700"},"source":["multi_counter = 0\n","for line in df.index:\n","    sentiment_list = []\n","    for col in pol_cols:\n","        if df.loc[line,col] != None:\n","            sentiment_list += [df.loc[line,col]]\n","    if len(set(sentiment_list)) > 1:\n","        multi_counter += 1\n","multi_counter"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"TA33P7z7pJ26"},"source":["# Remove \"conflict\""]},{"cell_type":"code","metadata":{"id":"PMYoki_owON1","executionInfo":{"status":"ok","timestamp":1624894833963,"user_tz":-120,"elapsed":630,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["for line in range(len(df)):\n","    for col in pol_cols:\n","        if df.loc[line,col] == \"conflict\":\n","            df.loc[line,col] = None\n","            number = col[-1:]\n","            df.loc[line,\"aspect_term_\"+str(number)] = None\n","            df.loc[line,\"aspect_to_\"+str(number)] = None\n","            df.loc[line,\"aspect_from_\"+str(number)] = None"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"O7yBbHY4zFNV","executionInfo":{"status":"ok","timestamp":1624894833964,"user_tz":-120,"elapsed":120,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"decbc694-251a-47e5-d190-c34f538f6b8c"},"source":["df"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","      <th>aspect_term_11</th>\n","      <th>aspect_polarity_11</th>\n","      <th>aspect_from_11</th>\n","      <th>aspect_to_11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>None</td>\n","      <td>The food was served promptly but the meal wasn...</td>\n","      <td>food</td>\n","      <td>neutral</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>served</td>\n","      <td>positive</td>\n","      <td>13</td>\n","      <td>19</td>\n","      <td>appetizers</td>\n","      <td>positive</td>\n","      <td>93</td>\n","      <td>103</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>None</td>\n","      <td>When I got home, there was a message on the ma...</td>\n","      <td>owner</td>\n","      <td>neutral</td>\n","      <td>64</td>\n","      <td>69</td>\n","      <td>waitress</td>\n","      <td>negative</td>\n","      <td>88</td>\n","      <td>96</td>\n","      <td>wine</td>\n","      <td>neutral</td>\n","      <td>125</td>\n","      <td>129</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","      <td>The Scene Shun Lee Palace is popular with midt...</td>\n","      <td>Scene</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>lunch</td>\n","      <td>neutral</td>\n","      <td>144</td>\n","      <td>149</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>None</td>\n","      <td>To both our surprise, this inquiry was interpr...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>80</td>\n","      <td>86</td>\n","      <td>pastries</td>\n","      <td>neutral</td>\n","      <td>145</td>\n","      <td>153</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>None</td>\n","      <td>In fact you can find their menu offerings at o...</td>\n","      <td>menu</td>\n","      <td>neutral</td>\n","      <td>27</td>\n","      <td>31</td>\n","      <td>prices</td>\n","      <td>positive</td>\n","      <td>89</td>\n","      <td>95</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>None</td>\n","      <td>The wait staff is slow, full of attitude and f...</td>\n","      <td>wait staff</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>116</td>\n","      <td>122</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>None</td>\n","      <td>Our waiter was inattentive throughout the meal...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>meal</td>\n","      <td>neutral</td>\n","      <td>42</td>\n","      <td>46</td>\n","      <td>dinner</td>\n","      <td>neutral</td>\n","      <td>72</td>\n","      <td>78</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>None</td>\n","      <td>In Short The much-copied look of hardwood floo...</td>\n","      <td>stained glass</td>\n","      <td>positive</td>\n","      <td>61</td>\n","      <td>74</td>\n","      <td>walls</td>\n","      <td>neutral</td>\n","      <td>113</td>\n","      <td>118</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>None</td>\n","      <td>Pair you food with the excellent beers on tap ...</td>\n","      <td>food</td>\n","      <td>neutral</td>\n","      <td>9</td>\n","      <td>13</td>\n","      <td>beers on tap</td>\n","      <td>positive</td>\n","      <td>33</td>\n","      <td>45</td>\n","      <td>priced wine list</td>\n","      <td>positive</td>\n","      <td>60</td>\n","      <td>76</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>None</td>\n","      <td>They were out of Hennessy, Alize, Mojitos, Pin...</td>\n","      <td>Mojitos</td>\n","      <td>neutral</td>\n","      <td>34</td>\n","      <td>41</td>\n","      <td>Pinot</td>\n","      <td>neutral</td>\n","      <td>43</td>\n","      <td>48</td>\n","      <td>drink list</td>\n","      <td>negative</td>\n","      <td>158</td>\n","      <td>168</td>\n","      <td>drinks</td>\n","      <td>negative</td>\n","      <td>238</td>\n","      <td>244</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 46 columns</p>\n","</div>"],"text/plain":["       id  ... aspect_to_11\n","0    None  ...         None\n","1    None  ...         None\n","2    None  ...         None\n","3    None  ...         None\n","4    None  ...         None\n","..    ...  ...          ...\n","495  None  ...         None\n","496  None  ...         None\n","497  None  ...         None\n","498  None  ...         None\n","499  None  ...         None\n","\n","[500 rows x 46 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"Yhf5gFid8__f"},"source":["# Check for wrong positions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nckKUb_U9BuI","executionInfo":{"status":"ok","timestamp":1624894833965,"user_tz":-120,"elapsed":118,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"956ba6c0-b816-4c99-a4a5-6ee517c5dddd"},"source":["mistakes = []\n","for ii in range(len(df)):\n","    for xx in range(1,aspect_number):\n","        asp_col = \"aspect_term_\"+str(xx)\n","        from_col = \"aspect_from_\"+str(xx)\n","        to_col = \"aspect_to_\"+str(xx)\n","        actual_term = df.loc[ii,asp_col]\n","        if actual_term != None:\n","            pos_term = df.text[ii][int(df.loc[ii,from_col]):int(df.loc[ii,to_col])]\n","            if actual_term != pos_term:\n","                mistakes += [ii]\n","                print(actual_term, pos_term)\n","mistakes"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"z8m9Y4Idr9Sa"},"source":["# Check for wrong aspect terms"]},{"cell_type":"code","metadata":{"id":"EB2miqWZnx_b","executionInfo":{"status":"ok","timestamp":1624894838156,"user_tz":-120,"elapsed":4198,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["import nltk"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OeQ1uqko757","executionInfo":{"status":"ok","timestamp":1624894838162,"user_tz":-120,"elapsed":39,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"482d2f0e-bccd-4895-afba-d25b18ef3970"},"source":["nltk.download('punkt')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"LiykKcbXnYXm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624894838163,"user_tz":-120,"elapsed":34,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"65aa8fd1-4bf4-4bbd-c391-31f1f9e1a4fe"},"source":["for ii in range(len(df)):\n","    tokens = nltk.word_tokenize(df.text[ii])\n","    for xx in range(1,aspect_number):\n","        actual_term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","        if actual_term != None:\n","            for asp_part in nltk.word_tokenize(actual_term):\n","                if asp_part not in tokens and asp_part+\"-\" not in tokens:\n","                    print(ii,\"-\",xx,\":\",tokens)\n","                    print(actual_term, asp_part)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["405 - 4 : ['I', 'love', 'the', 'food', 'here', ',', 'and', 'although', 'it', 'is', 'pricey', ',', 'the', 'entree', 'comes', 'with', 'rice', ',', 'naan', ',', 'dal', ',', 'and', 'salad', ',', 'which', 'makes', 'it', 'worthwhile', '.']\n","d al d\n","405 - 4 : ['I', 'love', 'the', 'food', 'here', ',', 'and', 'although', 'it', 'is', 'pricey', ',', 'the', 'entree', 'comes', 'with', 'rice', ',', 'naan', ',', 'dal', ',', 'and', 'salad', ',', 'which', 'makes', 'it', 'worthwhile', '.']\n","d al al\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fidv-SMbVZGv","executionInfo":{"status":"ok","timestamp":1624894838164,"user_tz":-120,"elapsed":31,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["df.loc[405,\"aspect_term_4\"] = \"dal\"\n","df.loc[405,\"aspect_from_4\"] = str(83)\n","df.loc[405,\"aspect_to_4\"] = str(86)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4890gf_IfDW"},"source":["Re-check for wrong positions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbRs6QssIfDY","executionInfo":{"status":"ok","timestamp":1624894838164,"user_tz":-120,"elapsed":29,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"fbe9b9ba-dfab-4c92-9804-9cdf336c8297"},"source":["mistakes = []\n","for ii in range(len(df)):\n","    for xx in range(1,aspect_number):\n","        asp_col = \"aspect_term_\"+str(xx)\n","        from_col = \"aspect_from_\"+str(xx)\n","        to_col = \"aspect_to_\"+str(xx)\n","        actual_term = df.loc[ii,asp_col]\n","        if actual_term != None:\n","            pos_term = df.text[ii][int(df.loc[ii,from_col]):int(df.loc[ii,to_col])]\n","            if actual_term != pos_term:\n","                mistakes += [ii]\n","                print(actual_term, pos_term)\n","mistakes"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"nErh__edRueT"},"source":["# Final Descriptive Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"dj87AyKpRueU"},"source":["## Aspects per sentence"]},{"cell_type":"code","metadata":{"id":"CEhrvKeBRueU","executionInfo":{"status":"ok","timestamp":1624894838165,"user_tz":-120,"elapsed":27,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["pol_cols = [\"aspect_polarity_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CmFSw57RueV","executionInfo":{"status":"ok","timestamp":1624894838167,"user_tz":-120,"elapsed":28,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"885c083b-6b7d-4f27-ef6d-3ccb15eb4e9a"},"source":["prev = 0\n","total_asp = 0\n","for no, col in enumerate(pol_cols):\n","\n","    if col != \"aspect_polarity_1\":\n","        print(\"sentences with exactly \", no, \"aspects:\", prev - sum(df[col].value_counts()))\n","        total_asp += no * (prev - sum(df[col].value_counts()))\n","\n","    if col == \"aspect_polarity_\"+str(aspect_number-1):\n","        print(\"sentences with exactly \", no+1, \"aspects:\", sum(df[col].value_counts()))\n","        total_asp += (no+1) * sum(df[col].value_counts())\n","\n","\n","    prev = sum(df[col].value_counts())\n","\n","print(\"total no of aspects: \", total_asp)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["sentences with exactly  1 aspects: 0\n","sentences with exactly  2 aspects: 264\n","sentences with exactly  3 aspects: 173\n","sentences with exactly  4 aspects: 45\n","sentences with exactly  5 aspects: 10\n","sentences with exactly  6 aspects: 5\n","sentences with exactly  7 aspects: 0\n","sentences with exactly  8 aspects: 1\n","sentences with exactly  9 aspects: 0\n","sentences with exactly  10 aspects: 1\n","sentences with exactly  11 aspects: 1\n","total no of aspects:  1336\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CYou9xluRueV"},"source":["## Sentiment Frequency"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2C9PeQYRueW","executionInfo":{"status":"ok","timestamp":1624894838168,"user_tz":-120,"elapsed":27,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"8b9f1c7f-c95a-4d9c-fe26-554e404163ac"},"source":["df_pol = df.loc[:,pol_cols]\n","df_pol_counts = df_pol.apply(pd.Series.value_counts)\n","df_pol_counts.sum(axis=1)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    329.0\n","neutral     607.0\n","positive    400.0\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cC82BXRlRueX","executionInfo":{"status":"ok","timestamp":1624894838170,"user_tz":-120,"elapsed":26,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"fdce49cf-9a07-4cf6-b941-b82e656fef33"},"source":["sum(df_pol_counts.sum(axis=1))"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1336.0"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"vGcQnp2rRueY"},"source":["## Sentences with more than one aspect"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVUz59lSRueY","executionInfo":{"status":"ok","timestamp":1624894838170,"user_tz":-120,"elapsed":23,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"b8b64bff-d610-4d6c-fc84-48b5df54c11c"},"source":["multi_counter = 0\n","for line in df.index:\n","    sentiment_list = []\n","    for col in pol_cols:\n","        if df.loc[line,col] != None:\n","            sentiment_list += [df.loc[line,col]]\n","    if len(set(sentiment_list)) > 1:\n","        multi_counter += 1\n","multi_counter"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"cpCOtYGA0OQd"},"source":["# Save as xml"]},{"cell_type":"code","metadata":{"id":"iYzlSnxhzl0N"},"source":["def xml_maker(df,aspect_number):\n","\n","    root = ET.Element('sentences')\n","\n","    for line in df.index:\n","        name = \"sentence\"\n","        entry = ET.SubElement(root, name)\n","        entry.set(\"id\", str(df[\"id\"][line]))\n","\n","        text_child = ET.SubElement(entry, \"text\")\n","        text_child.text = str(df[\"text\"][line])\n","\n","        asp_child = ET.SubElement(entry, \"aspectTerms\")\n","        for xx in range(1,aspect_number):\n","            if df.loc[line,\"aspect_term_\"+str(xx)] != None:\n","                asp_subchild = ET.SubElement(asp_child, \"aspectTerm\")\n","                asp_subchild.set(\"from\",str(df[\"aspect_from_\"+str(xx)][line]))\n","                asp_subchild.set(\"polarity\",str(df[\"aspect_polarity_\"+str(xx)][line]))\n","                asp_subchild.set(\"term\",str(df[\"aspect_term_\"+str(xx)][line]))\n","                asp_subchild.set(\"to\",str(df[\"aspect_to_\"+str(xx)][line]))\n","\n","    return ET.tostring(root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZ-FU7HtPq1H"},"source":["xml_data = xml_maker(df,aspect_number)\n","\n","with open(output_path+\"test.xml\",\"w\") as f:\n","    f.write(xml_data.decode('utf-8'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu9troUx3voZ"},"source":["# Create xml.seg"]},{"cell_type":"code","metadata":{"id":"8IvrB0Lf52zp"},"source":["def pol_to_no(sentiment):\n","  \n","    if sentiment == \"positive\":\n","        pol = 1\n","    elif sentiment == \"negative\":\n","        pol = -1\n","    elif sentiment == \"neutral\":\n","        pol = 0\n","\n","    return str(pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIXzzTPlRkwc"},"source":["def xml_seg_maker(df):\n","\n","    df_wo_id = df.drop(columns=\"id\", axis=1, inplace=False)\n","    df_wo_id.reset_index(drop=True, inplace=True)\n","    data_lines = []\n","\n","    for ii in df_wo_id.index:\n","\n","        line = list(df_wo_id.loc[ii])\n","        o_text = line[0]\n","        aspects = [line[xx] for xx in range(1,len(df_wo_id.loc[0]),4) if line[xx] != None]\n","        pols = [line[xx] for xx in range(2,len(df_wo_id.loc[0]),4) if line[xx] != None]\n","\n","        for asp in range(len(aspects)):\n","            text = o_text.replace(aspects[asp],'$T$')\n","            pol = pol_to_no(pols[asp])\n","\n","            data_lines += [text,aspects[asp],pol]\n","    \n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ND8pnKtwCLZL"},"source":["xml_seg_data = xml_seg_maker(df)\n","\n","with open(output_path+\"test.xml.seg\",\"w\") as f:\n","    f.write('\\n'.join(xml_seg_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zY4NaEVH1Dhi"},"source":["# Create BERT+txt"]},{"cell_type":"code","metadata":{"id":"7c_Zb3Lk1vUa"},"source":["asp_cols = [\"aspect_term_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kopkDBAmmkxV"},"source":["def sent_conv(sentiment):\n","\n","    if sentiment == \"positive\":\n","        return \"POS\"\n","    elif sentiment == \"negative\":\n","        return \"NEG\"\n","    elif sentiment == \"neutral\":\n","        return \"NEU\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8E6pSYzt08i"},"source":["def txt_maker(df):\n","\n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.loc[line,\"text\"]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = sent_conv(df.loc[line,pol_cols[col]])\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for tok in tokens:\n","            if tok in asp_sent_dict.keys():\n","                label += tok + \"=T-\" + asp_sent_dict[tok] + \" \"\n","            else:\n","                label += tok + \"=O \"\n","        label = label[:-1]\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok in asp_sent_dict.keys():\n","                    new_pol = asp_sent_dict[new_tok]\n","                    old_label = \" \".join([tokens[no+xx]+\"=O\" for xx in range(ii) if no+xx < len(tokens)])\n","                    new_label = \" \".join([tokens[no+xx]+\"=T-\"+new_pol for xx in range(ii) if no+xx < len(tokens)])\n","                    label = label.replace(old_label, new_label)\n","  \n","        data_lines += [text+\"####\"+label]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvTsu2IyupwT"},"source":["txt_data = txt_maker(df)\n","\n","with open(output_path+\"test.txt\",\"w\") as f:\n","    f.write('\\n'.join(txt_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4i-GN5Q1KlZ"},"source":["# Create RGATjson"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCyhkwcKwClh","executionInfo":{"status":"ok","timestamp":1620648381346,"user_tz":-120,"elapsed":31394,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"6a9a6a4f-ee9e-4173-daac-51a1dd350025"},"source":["!pip install stanza"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting stanza\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n","\r\u001b[K     |█▏                              | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 26.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 28.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 18.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 17.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (56.1.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n","Installing collected packages: stanza\n","Successfully installed stanza-1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Le-5FSclw2rm"},"source":["import stanza"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hp6-9OkNwJ1e","executionInfo":{"status":"ok","timestamp":1620648468239,"user_tz":-120,"elapsed":118277,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"425271e6-9e1e-434d-bc75-aadfdeed1392"},"source":["stanza.download('en')\n","nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 24.9MB/s]                    \n","2021-05-10 12:06:24 INFO: Downloading default packages for language: en (English)...\n","Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:15<00:00, 5.47MB/s]\n","2021-05-10 12:07:46 INFO: Finished downloading models and saved to /root/stanza_resources.\n","2021-05-10 12:07:46 INFO: Loading these models for language: en (English):\n","========================\n","| Processor | Package  |\n","------------------------\n","| tokenize  | combined |\n","| pos       | combined |\n","| lemma     | combined |\n","| depparse  | combined |\n","========================\n","\n","2021-05-10 12:07:46 INFO: Use device: cpu\n","2021-05-10 12:07:46 INFO: Loading: tokenize\n","2021-05-10 12:07:46 INFO: Loading: pos\n","2021-05-10 12:07:46 INFO: Loading: lemma\n","2021-05-10 12:07:47 INFO: Loading: depparse\n","2021-05-10 12:07:47 INFO: Done loading processors!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IBmBwPg_yWGU"},"source":["from more_itertools import locate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvGzY7ydRUxU"},"source":["## in case of creating for the first time"]},{"cell_type":"code","metadata":{"id":"fGo5KiewzkCo"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILMTmvCG-sY6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620643359846,"user_tz":-120,"elapsed":312520,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"534e5261-dab7-487a-d6e5-6178e5af384c"},"source":["'''\n","def json_make_pos(df):\n","   \n","    manual_pos = {}\n","\n","    for ii in df.index:\n","        \n","        new_dict = {}\n","        text = df.loc[ii,\"text\"]\n","        tokens = [token.text for sentence in nlp(text).sentences for token in sentence.tokens]\n","        \n","        new_dict[\"aspects\"] = []\n","        for xx in range(1,aspect_number):         \n","            if df.loc[ii,\"aspect_term_\"+str(xx)] != None:\n","                asp_dict = {}   \n","                term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","\n","                # construct aspect position on token level\n","                asp_toks = [token.text for sentence in nlp(term).sentences for token in sentence.tokens]\n","                asp_ind = [list(locate(tokens, lambda a: a == tok)) for tok in asp_toks]\n","\n","                # for aspects appearing only once in text, take the correct position\n","                # otherwise set to None\n","                if len(asp_ind[0]) == 1:\n","                    from_index = asp_ind[0][0]\n","                else:\n","                    from_index = None\n","                if len(asp_ind[-1]) == 1:\n","                    to_index = asp_ind[-1][0] \n","                else: \n","                    to_index = None\n","\n","                # if both start and end pos are unknown, \n","                # e.g. for single-word aspects, \n","                # take character positions for help\n","                if from_index == None and to_index == None and len(asp_ind[0]) != 0:\n","                    print(ii, \": \", text)\n","                    print(\"original term: \",term)\n","                    all_char_from = [i for i in range(len(text)) if text.startswith(asp_toks[0], i)]\n","                    print(\"all start chars: \",all_char_from)\n","                    corr_char_from = int(df.loc[ii, \"aspect_from_\"+str(xx)])\n","                    print(\"correct start char: \",corr_char_from)\n","                    print(\"text beginning at correct start char: \", text[corr_char_from:])\n","                    print(\"tokens: \",tokens)\n","                    print(\"original asp tokens: \", asp_toks)\n","                    print(\"original asp indices: \",asp_ind)\n","                    if corr_char_from == max(all_char_from):\n","                        from_index = asp_ind[0][-1]\n","                    elif corr_char_from == min(all_char_from):\n","                        from_index = asp_ind[0][0]\n","\n","                # in case of missing start/end positions,\n","                # try to find \"to\"/\"from\" using aspect token number as distance\n","                if from_index == None and to_index != None:\n","                    from_index = to_index - len(asp_toks) +1\n","                if to_index == None and from_index != None:\n","                    to_index = from_index + len(asp_toks) -1\n","\n","                # correct tokenization errors in aspect term tokenization\n","                if from_index == None or to_index == None or asp_toks != tokens[from_index:to_index+1]:\n","\n","                    print(\"Tokenization Error in line \",ii,\"!\")\n","                    for pos, tok in enumerate(tokens):\n","                        print(pos, tok)\n","                    print(\"aspect term: \", term)\n","                    print(\"original asp tokens: \", asp_toks)\n","\n","                    from_index = int(input(\"start position?\"))\n","                    to_index = int(input(\"end position?\"))\n","\n","                    # add manually stated positions to dict for reproducibility\n","                    manual_pos[text] = {}\n","                    manual_pos[text][term] = {}\n","                    manual_pos[text][term][\"from\"] = from_index\n","                    manual_pos[text][term][\"to\"] = to_index\n","                    print(manual_pos)\n","\n","                    print(\"Final aspect tokens: \", tokens[from_index:to_index+1])\n","\n","                asp_dict[\"from\"] = from_index\n","                asp_dict[\"to\"] = to_index + 1\n","\n","    with open(aspect_path+\"mams_test.json\",\"w\") as f:\n","        json.dump(manual_pos, f)\n","\n","json_make_pos(df)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokenization Error in line  8 !\n","0 The\n","1 brasserie-style\n","2 menu\n","3 remains\n","4 relatively\n","5 unchanged\n","6 ,\n","7 featuring\n","8 classic\n","9 bistro\n","10 choices\n","11 like\n","12 frisee\n","13 salad\n","14 with\n","15 bacon\n","16 ,\n","17 blue\n","18 cheese\n","19 and\n","20 a\n","21 poached\n","22 egg\n","23 ,\n","24 steak\n","25 tartare\n","26 ,\n","27 moules\n","28 and\n","29 steak\n","30 frites\n","31 ,\n","32 and\n","33 various\n","34 burgers\n","35 and\n","36 sandwiches\n","37 .\n","aspect term:  brasserie-style menu\n","original asp tokens:  ['brasserie-', 'style', 'menu']\n","start position?1\n","end position?2\n","{'The brasserie-style menu remains relatively unchanged, featuring classic bistro choices like frisee salad with bacon, blue cheese and a poached egg, steak tartare, moules and steak frites, and various burgers and sandwiches.': {'brasserie-style menu': {'from': 1, 'to': 2}}}\n","Final aspect tokens:  ['brasserie-style', 'menu']\n","Tokenization Error in line  99 !\n","0 Start\n","1 with\n","2 fennel\n","3 -\n","4 fragrant\n","5 grilled\n","6 sardines\n","7 and\n","8 the\n","9 extraordinary\n","10 pastas\n","11 ,\n","12 such\n","13 as\n","14 poppy\n","15 -seeded\n","16 sweet\n","17 beet\n","18 ravioli\n","19 ,\n","20 brown\n","21 -\n","22 butter\n","23 -\n","24 slicked\n","25 squash\n","26 tortelli\n","27 ,\n","28 and\n","29 airy\n","30 swiss\n","31 chard\n","32 -\n","33 ricotta\n","34 malfatti\n","35 (\n","36 misshapen\n","37 gnocchi\n","38 with\n","39 fried\n","40 sage\n","41 )\n","42 .\n","aspect term:  brown-butter-slicked squash tortelli\n","original asp tokens:  ['brown-butter', '-slicked', 'squash', 'tortelli']\n","start position?20\n","end position?26\n","{'The brasserie-style menu remains relatively unchanged, featuring classic bistro choices like frisee salad with bacon, blue cheese and a poached egg, steak tartare, moules and steak frites, and various burgers and sandwiches.': {'brasserie-style menu': {'from': 1, 'to': 2}}, 'Start with fennel-fragrant grilled sardines and the extraordinary pastas, such as poppy-seeded sweet beet ravioli, brown-butter-slicked squash tortelli, and airy swiss chard-ricotta malfatti (misshapen gnocchi with fried sage).': {'brown-butter-slicked squash tortelli': {'from': 20, 'to': 26}}}\n","Final aspect tokens:  ['brown', '-', 'butter', '-', 'slicked', 'squash', 'tortelli']\n","Tokenization Error in line  107 !\n","0 The\n","1 Food\n","2 The\n","3 versatile\n","4 Austrian\n","5 menu\n","6 ,\n","7 fused\n","8 with\n","9 French\n","10 and\n","11 American\n","12 ,\n","13 rewards\n","14 both\n","15 high\n","16 and\n","17 low\n","18 -\n","19 end\n","20 appetites\n","21 :\n","22 rich\n","23 ,\n","24 cheese\n","25 -\n","26 laden\n","27 onion\n","28 soup\n","29 ,\n","30 grilled\n","31 Gruyere\n","32 and\n","33 country\n","34 ham\n","35 sandwiches\n","36 and\n","37 big-bunned\n","38 burgers\n","39 are\n","40 as\n","41 satisfying\n","42 as\n","43 velvety\n","44 liver\n","45 terrine\n","46 with\n","47 kumquat\n","48 -\n","49 cranberry\n","50 compote\n","51 ,\n","52 speck\n","53 and\n","54 sausage\n","55 charcuterie\n","56 and\n","57 inexpensive\n","58 cheese\n","59 plates\n","60 .\n","aspect term:  kumquat-cranberry\n","original asp tokens:  ['kumquat-cranberry']\n","start position?47\n","end position?49\n","{'The brasserie-style menu remains relatively unchanged, featuring classic bistro choices like frisee salad with bacon, blue cheese and a poached egg, steak tartare, moules and steak frites, and various burgers and sandwiches.': {'brasserie-style menu': {'from': 1, 'to': 2}}, 'Start with fennel-fragrant grilled sardines and the extraordinary pastas, such as poppy-seeded sweet beet ravioli, brown-butter-slicked squash tortelli, and airy swiss chard-ricotta malfatti (misshapen gnocchi with fried sage).': {'brown-butter-slicked squash tortelli': {'from': 20, 'to': 26}}, 'The Food The versatile Austrian menu, fused with French and American, rewards both high and low-end appetites: rich, cheese-laden onion soup, grilled Gruyere and country ham sandwiches and big-bunned burgers are as satisfying as velvety liver terrine with kumquat-cranberry compote, speck and sausage charcuterie and inexpensive cheese plates.': {'kumquat-cranberry': {'from': 47, 'to': 49}}}\n","Final aspect tokens:  ['kumquat', '-', 'cranberry']\n","289 :  The server came by only once to pour additional wine for the table; the rest of the time, we had to fish the bottle out of the two-table communal bucket ourselves.\n","original term:  the table\n","all start chars:  [57, 68, 80, 105, 123]\n","correct start char:  57\n","text beginning at correct start char:  the table; the rest of the time, we had to fish the bottle out of the two-table communal bucket ourselves.\n","tokens:  ['The', 'server', 'came', 'by', 'only', 'once', 'to', 'pour', 'additional', 'wine', 'for', 'the', 'table', ';', 'the', 'rest', 'of', 'the', 'time', ',', 'we', 'had', 'to', 'fish', 'the', 'bottle', 'out', 'of', 'the', 'two', '-', 'table', 'communal', 'bucket', 'ourselves', '.']\n","original asp tokens:  ['the', 'table']\n","original asp indices:  [[11, 14, 17, 24, 28], [12, 31]]\n","Tokenization Error in line  463 !\n","0 Amusing\n","1 details\n","2 distinguish\n","3 desserts\n","4 ,\n","5 from\n","6 dulce\n","7 de\n","8 leche\n","9 ice-\n","10 cream\n","11 profiteroles\n","12 dotting\n","13 a\n","14 chocolate\n","15 sauce\n","16 tic-tac\n","17 -\n","18 toe\n","19 board\n","20 ,\n","21 to\n","22 coconut\n","23 custard\n","24 surrounded\n","25 by\n","26 a\n","27 sea\n","28 of\n","29 Malibu-rum\n","30 gelee\n","31 and\n","32 poached\n","33 pineapple\n","34 .\n","aspect term:  chocolate sauce tic-tac-toe\n","original asp tokens:  ['chocolate', 'sauce', 'tic-tac-toe']\n","start position?14\n","end position?18\n","{'The brasserie-style menu remains relatively unchanged, featuring classic bistro choices like frisee salad with bacon, blue cheese and a poached egg, steak tartare, moules and steak frites, and various burgers and sandwiches.': {'brasserie-style menu': {'from': 1, 'to': 2}}, 'Start with fennel-fragrant grilled sardines and the extraordinary pastas, such as poppy-seeded sweet beet ravioli, brown-butter-slicked squash tortelli, and airy swiss chard-ricotta malfatti (misshapen gnocchi with fried sage).': {'brown-butter-slicked squash tortelli': {'from': 20, 'to': 26}}, 'The Food The versatile Austrian menu, fused with French and American, rewards both high and low-end appetites: rich, cheese-laden onion soup, grilled Gruyere and country ham sandwiches and big-bunned burgers are as satisfying as velvety liver terrine with kumquat-cranberry compote, speck and sausage charcuterie and inexpensive cheese plates.': {'kumquat-cranberry': {'from': 47, 'to': 49}}, 'Amusing details distinguish desserts, from dulce de leche ice-cream profiteroles dotting a chocolate sauce tic-tac-toe board, to coconut custard surrounded by a sea of Malibu-rum gelee and poached pineapple.': {'chocolate sauce tic-tac-toe': {'from': 14, 'to': 18}}}\n","Final aspect tokens:  ['chocolate', 'sauce', 'tic-tac', '-', 'toe']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A9j2SYYxWT1y"},"source":["## in case of reproducing the dataset"]},{"cell_type":"code","metadata":{"id":"LbLhvcZ_Yag5"},"source":["def json_maker(df, manual_pos):\n","   \n","    new_data = []\n","\n","    for ii in df.index:\n","        \n","        new_dict = {}\n","        text = df.loc[ii,\"text\"]\n","\n","        tokens = [token.text for sentence in nlp(text).sentences for token in sentence.tokens]\n","        new_dict[\"token\"] = tokens\n","\n","        new_dict[\"pos\"] = [word.xpos for sentence in nlp(text).sentences for word in sentence.words]\n","        new_dict[\"head\"] = [str(word.head) for sentence in nlp(text).sentences for word in sentence.words]\n","        new_dict[\"deprel\"] = [word.deprel for sentence in nlp(text).sentences for word in sentence.words]\n","        \n","        new_dict[\"aspects\"] = []\n","        for xx in range(1,aspect_number):         \n","            if df.loc[ii,\"aspect_term_\"+str(xx)] != None:\n","                asp_dict = {}   \n","                term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","                asp_dict[\"term\"] = term\n","                asp_dict[\"polarity\"] = df.loc[ii,\"aspect_polarity_\"+str(xx)]\n","\n","                # construct aspect position on token level\n","                asp_toks = [token.text for sentence in nlp(term).sentences for token in sentence.tokens]\n","                asp_ind = [list(locate(tokens, lambda a: a == term)) for term in asp_toks]\n","\n","                # for aspects appearing only once in text, take the correct position\n","                # otherwise set to None\n","                if len(asp_ind[0]) == 1:\n","                    from_index = asp_ind[0][0]\n","                else:\n","                    from_index = None\n","                if len(asp_ind[-1]) == 1:\n","                    to_index = asp_ind[-1][0] \n","                else: \n","                    to_index = None\n","\n","                # if both start and end pos are unknown, \n","                # e.g. for single-word aspects, \n","                # take character positions for help\n","                if from_index == None and to_index == None and len(asp_ind[0]) != 0:\n","\n","                    all_char_from = [i for i in range(len(text)) if text.startswith(asp_toks[0], i)]\n","                    corr_char_from = int(df.loc[ii, \"aspect_from_\"+str(xx)])\n","\n","                    if corr_char_from == max(all_char_from):\n","                        from_index = asp_ind[0][-1]\n","                    elif corr_char_from == min(all_char_from):\n","                        from_index = asp_ind[0][0]\n","\n","                # in case of missing start/end positions,\n","                # try to find \"to\"/\"from\" using aspect token number as distance\n","                if from_index == None and to_index != None:\n","                    from_index = to_index - len(asp_toks) +1\n","                if to_index == None and from_index != None:\n","                    to_index = from_index + len(asp_toks) -1\n","\n","                # correct tokenization errors in aspect term tokenization\n","                if from_index == None or to_index == None or asp_toks != tokens[from_index:to_index+1]:\n","                    if text in manual_pos.keys() and term in manual_pos[text].keys():\n","                        from_index = manual_pos[text][term][\"from\"]\n","                        to_index = manual_pos[text][term][\"to\"]\n","\n","                asp_dict[\"from\"] = from_index\n","                asp_dict[\"to\"] = to_index + 1\n","\n","                new_dict[\"aspects\"] += [asp_dict]\n","        \n","        new_data += [new_dict]\n","\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbpMtyJ5XVXi"},"source":["pos_obj = open(aspect_path+\"mams_test.json\")\n","loaded_pos = json.load(pos_obj)\n","\n","json_data = json_maker(df, loaded_pos)\n","\n","with open(output_path+\"test.json\",\"w\") as f:\n","    json.dump(json_data, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeHXUSmyaRfA"},"source":["# Create LCF-ATEPCdat"]},{"cell_type":"code","metadata":{"id":"si0ea14Q4PVZ"},"source":["def pol_to_no_shifted(sentiment):\n","  \n","    if sentiment == \"positive\":\n","        pol = 2\n","    elif sentiment == \"negative\":\n","        pol = 0\n","    elif sentiment == \"neutral\":\n","        pol = 1\n","\n","    return str(pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z0s04TbaVXo"},"source":["def dat_maker(df):\n","    \n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.loc[line,\"text\"]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = pol_to_no_shifted(df.loc[line,pol_cols[col]])\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for tok in tokens:\n","            if tok in asp_sent_dict.keys():\n","                label += tok + \" B-ASP -1\\n\"\n","            else:\n","                label += tok + \" O -1\\n\"\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok not in tokens and new_tok in asp_sent_dict.keys():\n","                    label = label.replace(tokens[no]+\" O -1\",tokens[no]+\" B-ASP -1\")\n","                    for xx in range(1,ii):\n","                        label = label.replace(tokens[no+xx]+\" O -1\",tokens[no+xx]+\" I-ASP -1\")\n","\n","        # create duplicates of review in case of more than one aspect\n","        for key, val in asp_sent_dict.items():\n","            if key in tokens:\n","                new_label = label.replace(key+\" B-ASP -1\", key+\" B-ASP \"+val)\n","                data_lines += [new_label]\n","                data_lines += [\"\\n\"]\n","            else:\n","                for ii in range(2,max_asp_len+1):\n","                    for no,tok in enumerate(tokens):\n","                        new_tok = \" \".join(tokens[no:no+ii])\n","                        if new_tok == key:\n","                            new_label = label.replace(tokens[no]+\" B-ASP -1\",tokens[no]+\" B-ASP \"+val)\n","                            for xx in range(1,ii):\n","                                new_label = new_label.replace(tokens[no+xx]+\" I-ASP -1\",tokens[no+xx]+\" I-ASP \"+val)\n","\n","                            data_lines += [new_label]\n","                            data_lines += [\"\\n\"]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n87vqlsKa36r"},"source":["dat_data = dat_maker(df)\n","\n","with open(output_path+\"test.dat\",\"w\") as f:\n","    f.write(''.join(dat_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zccA_AJz1ZW4"},"source":["# Create GRACEtxt"]},{"cell_type":"code","metadata":{"id":"Jht_ESNPhmOY"},"source":["asp_cols = [\"aspect_term_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORfnB8CS6qjN"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce4Aqwxh7VGF","executionInfo":{"status":"ok","timestamp":1621531629364,"user_tz":-120,"elapsed":556,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"a792c177-b474-403e-b10e-a83fab0e1192"},"source":["nltk.download('averaged_perceptron_tagger')\n","nltk.download('conll2000')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"al0gF6NL1cdd"},"source":["Create chunks and pos tags. Source: https://towardsdatascience.com/chunking-in-nlp-decoded-b4a71b2b4e24\n"]},{"cell_type":"code","metadata":{"id":"z-KWOrnf_5hn"},"source":["from nltk.tag import UnigramTagger, BigramTagger\n","from nltk.chunk import ChunkParserI\n","from nltk.chunk.util import tree2conlltags, conlltags2tree\n","from nltk.corpus import conll2000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0s_64z7oaF"},"source":["def conll_tag_chunks(chunk_sents):\n","    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n","    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n","    \n","def combined_tagger(train_data, taggers, backoff=None):\n","    for tagger in taggers:\n","        backoff = tagger(train_data, backoff=backoff)\n","    return backoff\n","\n","class NGramTagChunker(ChunkParserI):\n","\n","    def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\n","        train_sent_tags=conll_tag_chunks(train_sentences)\n","        self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)\n","    \n","    def parse(self,tagged_sentence):\n","        if not tagged_sentence:\n","            return None\n","        pos_tags=[tag for word, tag in tagged_sentence]\n","        chunk_pos_tags=self.chunk_tagger.tag(pos_tags)\n","        chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\n","        wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]\n","        return conlltags2tree(wpc_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XBO64Z0ANbJ"},"source":["data = conll2000.chunked_sents()\n","ntc = NGramTagChunker(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUp02s4XBOY3"},"source":["Convert tags into BIOES scheme. Source: https://gist.github.com/allanj/5ad206f7f4645c0269b68fb2065712f4"]},{"cell_type":"code","metadata":{"id":"ZEtfTq5dC5Za"},"source":["def iob_iobes(tags):\n","    \"\"\"\n","    IOB2 (BIO) -> IOBES\n","    \"\"\"\n","    new_tags = []\n","    for i, tag in enumerate(tags):\n","        if tag == 'O':\n","            new_tags.append(tag)\n","        elif tag.split('-')[0] == 'B':\n","            if i + 1 != len(tags) and \\\n","                    tags[i + 1].split('-')[0] == 'I':\n","                new_tags.append(tag)\n","            else:\n","                new_tags.append(tag.replace('B-', 'S-'))\n","        elif tag.split('-')[0] == 'I':\n","            if i + 1 < len(tags) and \\\n","                    tags[i + 1].split('-')[0] == 'I':\n","                new_tags.append(tag)\n","            else:\n","                new_tags.append(tag.replace('I-', 'E-'))\n","        else:\n","            raise Exception('Invalid IOB format!')\n","    return new_tags"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQAlwhpqRwWS"},"source":["def grace_txt_maker(df):\n","    \n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.text[line]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create pos tags\n","        pos_tags = nltk.pos_tag(tokens)\n","\n","        # create chunk/phrase tags\n","        full_tags = tree2conlltags(ntc.parse(pos_tags))\n","        chunks_list = [full_tags[ii][2] for ii in range(len(full_tags))]\n","        new_chunks = iob_iobes(chunks_list)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = df.loc[line,pol_cols[col]].upper()\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for pos,tok in enumerate(tokens):\n","            label += tok + \" \" + pos_tags[pos][1] + \" \" + new_chunks[pos]\n","            if tok in asp_sent_dict.keys():\n","                label +=  \" B_AP \" + asp_sent_dict[tok] + \" B_AP+\" + asp_sent_dict[tok] + \"\\n\"\n","            else:\n","                label += \" O O O \\n\"\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok not in tokens and new_tok in asp_sent_dict.keys():\n","                    new_pol = asp_sent_dict[new_tok]\n","                    label = label.replace(tokens[no]+ \" \" + pos_tags[no][1] + \" \" + new_chunks[no] + \" O O O \\n\",\n","                                          tokens[no]+ \" \" + pos_tags[no][1] + \" \" + new_chunks[no] + \" B_AP \" + \\\n","                                          new_pol + \" B_AP+\" + new_pol + \"\\n\")\n","                    for xx in range(1,ii):\n","                        label = label.replace(tokens[no+xx] + \" \" + pos_tags[no+xx][1] + \" \" + new_chunks[no+xx] + \" O O O \\n\",\n","                                          tokens[no+xx]+ \" \" + pos_tags[no+xx][1] + \" \" + new_chunks[no+xx] + \" I_AP \" + \\\n","                                          new_pol + \" I_AP+\" + new_pol + \"\\n\")\n","\n","        data_lines += [label]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jCIhax6erbg"},"source":["grace_txt_data = grace_txt_maker(df)\n","\n","with open(output_path+\"grace_test.txt\",\"w\") as f:\n","    f.write('\\n'.join(grace_txt_data))"],"execution_count":null,"outputs":[]}]}