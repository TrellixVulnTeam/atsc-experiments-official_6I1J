{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_mams-val.ipynb","provenance":[{"file_id":"15JlKkhGtbIHPXZBPynwMRKQcExWvqnRT","timestamp":1618499328888},{"file_id":"1jwSjHyEPiHzovPM5fjtYliW8YuJsC5NU","timestamp":1618497973114},{"file_id":"13Uuiig-8uZ_Bkg6NssQEOLujDkx4-yJ0","timestamp":1618494402410},{"file_id":"1HZRcbuLT7dKHVL_c5AXMj8cpXNUQz8Cx","timestamp":1618482437523},{"file_id":"10uitUAMMugkT4GzWH8-Fym8W-jGtkt1-","timestamp":1618481070596},{"file_id":"17vlkZFBPb3QsvbWkBLlDQG_QeqO-JZdm","timestamp":1618473918711},{"file_id":"1AT_0GiUm-x0jiLImp6kBPj2bJliUU7u9","timestamp":1612030190493}],"collapsed_sections":[],"authorship_tag":"ABX9TyN/H71wqSOaVw8sGsyC4vl8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TF99dpCJYsr","executionInfo":{"status":"ok","timestamp":1624894585693,"user_tz":-120,"elapsed":14838,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"e48a9186-53be-47f1-bde8-c74c4559ea90"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D3r8d-AzWGcf"},"source":["# Data Import"]},{"cell_type":"code","metadata":{"id":"1HsS1-jUzDZp","executionInfo":{"status":"ok","timestamp":1624894585695,"user_tz":-120,"elapsed":8,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["import pandas as pd\n","import xml.etree.ElementTree as ET"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"12vjxMVvzMR2","executionInfo":{"status":"ok","timestamp":1624894588915,"user_tz":-120,"elapsed":3226,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["parsedXML = ET.parse(\"/content/drive/My Drive/Masterarbeit/Data/Original/MAMS/mams_val.xml\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5n6BloJzkKq7","executionInfo":{"status":"ok","timestamp":1624894588916,"user_tz":-120,"elapsed":11,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["aspect_number = 11"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P07rAjP_lNXG","executionInfo":{"status":"ok","timestamp":1624894590625,"user_tz":-120,"elapsed":1718,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["dfcols = ['id','text']\n","for ii in range(1,aspect_number):\n","    dfcols.append(\"aspect_term_{}\".format(ii))\n","    dfcols.append(\"aspect_polarity_{}\".format(ii))\n","    dfcols.append(\"aspect_from_{}\".format(ii))\n","    dfcols.append(\"aspect_to_{}\".format(ii))          \n","df = pd.DataFrame(columns=dfcols)\n","\n","for sentence in parsedXML.getroot():\n","    id = sentence.attrib.get('id')\n","    text = sentence.find('text').text\n","    line = [id,text]\n","\n","    for asp in sentence.iter('aspectTerm'):\n","        term = asp.attrib.get(\"term\")\n","        pol = asp.attrib.get(\"polarity\")\n","        a_from = asp.attrib.get(\"from\")\n","        a_to = asp.attrib.get(\"to\")\n","\n","        line += [term, pol, a_from, a_to]\n","\n","    if len(line) < len(dfcols):\n","        pads = [None] * (len(dfcols)-len(line))\n","        line += pads\n","    \n","    df = df.append(pd.Series(line, index=dfcols), ignore_index=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"nsuk3xIs5UUE","executionInfo":{"status":"ok","timestamp":1624894590626,"user_tz":-120,"elapsed":25,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"2f993862-ebed-4388-c699-0171beee6f36"},"source":["df"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>None</td>\n","      <td>After a couple of drinks, the apps--I like the...</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>18</td>\n","      <td>24</td>\n","      <td>roll</td>\n","      <td>positive</td>\n","      <td>68</td>\n","      <td>72</td>\n","      <td>cripsy squid</td>\n","      <td>positive</td>\n","      <td>81</td>\n","      <td>93</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>None</td>\n","      <td>The basil pepper mojito was a little daunting ...</td>\n","      <td>basil pepper mojito</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>23</td>\n","      <td>flavor</td>\n","      <td>positive</td>\n","      <td>85</td>\n","      <td>91</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","      <td>Had to constantly ask the waiter to top up wat...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>26</td>\n","      <td>32</td>\n","      <td>water glasses</td>\n","      <td>neutral</td>\n","      <td>43</td>\n","      <td>56</td>\n","      <td>service</td>\n","      <td>positive</td>\n","      <td>72</td>\n","      <td>79</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>None</td>\n","      <td>The portions were so small that we still wante...</td>\n","      <td>portions</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>12</td>\n","      <td>dinner</td>\n","      <td>neutral</td>\n","      <td>61</td>\n","      <td>67</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>None</td>\n","      <td>The staff is very kind and well trained, they'...</td>\n","      <td>staff</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>97</td>\n","      <td>100</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>109</td>\n","      <td>115</td>\n","      <td>menu</td>\n","      <td>neutral</td>\n","      <td>156</td>\n","      <td>160</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>None</td>\n","      <td>Spinache rolls have lots of garlic.</td>\n","      <td>rolls</td>\n","      <td>neutral</td>\n","      <td>9</td>\n","      <td>14</td>\n","      <td>garlic</td>\n","      <td>positive</td>\n","      <td>28</td>\n","      <td>34</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>None</td>\n","      <td>I use to always get the turkey club, until rec...</td>\n","      <td>turkey club</td>\n","      <td>neutral</td>\n","      <td>24</td>\n","      <td>35</td>\n","      <td>bread</td>\n","      <td>positive</td>\n","      <td>77</td>\n","      <td>82</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>None</td>\n","      <td>The decor is worth a mention, with plush seati...</td>\n","      <td>decor</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>71</td>\n","      <td>74</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>None</td>\n","      <td>The Food The menu is better suited to the snac...</td>\n","      <td>Food</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>snacking</td>\n","      <td>positive</td>\n","      <td>42</td>\n","      <td>50</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>63</td>\n","      <td>66</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>None</td>\n","      <td>Another favorite is the hanger steak which is ...</td>\n","      <td>hanger steak</td>\n","      <td>positive</td>\n","      <td>24</td>\n","      <td>36</td>\n","      <td>amount of arugula</td>\n","      <td>positive</td>\n","      <td>101</td>\n","      <td>118</td>\n","      <td>a balsamic vinegar reduction served</td>\n","      <td>neutral</td>\n","      <td>123</td>\n","      <td>158</td>\n","      <td>sauce</td>\n","      <td>neutral</td>\n","      <td>164</td>\n","      <td>169</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 42 columns</p>\n","</div>"],"text/plain":["       id  ... aspect_to_10\n","0    None  ...         None\n","1    None  ...         None\n","2    None  ...         None\n","3    None  ...         None\n","4    None  ...         None\n","..    ...  ...          ...\n","495  None  ...         None\n","496  None  ...         None\n","497  None  ...         None\n","498  None  ...         None\n","499  None  ...         None\n","\n","[500 rows x 42 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"863AbS67LrD2"},"source":["# Drop duplicates"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"16yC8XZHoisI","executionInfo":{"status":"ok","timestamp":1624894590627,"user_tz":-120,"elapsed":22,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"b868a030-ee71-45ff-debf-ca5a35f7b1ed"},"source":["cols_wo_id = df.columns\n","cols_wo_id = cols_wo_id.drop(\"id\")\n","df[df.duplicated(cols_wo_id, keep=False)].sort_values(\"text\")"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [id, text, aspect_term_1, aspect_polarity_1, aspect_from_1, aspect_to_1, aspect_term_2, aspect_polarity_2, aspect_from_2, aspect_to_2, aspect_term_3, aspect_polarity_3, aspect_from_3, aspect_to_3, aspect_term_4, aspect_polarity_4, aspect_from_4, aspect_to_4, aspect_term_5, aspect_polarity_5, aspect_from_5, aspect_to_5, aspect_term_6, aspect_polarity_6, aspect_from_6, aspect_to_6, aspect_term_7, aspect_polarity_7, aspect_from_7, aspect_to_7, aspect_term_8, aspect_polarity_8, aspect_from_8, aspect_to_8, aspect_term_9, aspect_polarity_9, aspect_from_9, aspect_to_9, aspect_term_10, aspect_polarity_10, aspect_from_10, aspect_to_10]\n","Index: []"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"7VcO-JpTt4ev","executionInfo":{"status":"ok","timestamp":1624894590628,"user_tz":-120,"elapsed":19,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["df.drop_duplicates(cols_wo_id, ignore_index=True,inplace=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42Horu_479nq","executionInfo":{"status":"ok","timestamp":1624894590629,"user_tz":-120,"elapsed":18,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"65c1f0a9-f4fa-4160-feb2-7e69f836763b"},"source":["text_counts = df.text.value_counts()\n","text_counts[:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["The waitress at the bar was very nasty to me because she mistakenly took an order for thai ice tea from me when I asked for thai lemonade in a to-go cup.                               1\n","I wouldn't go for dinner though cause it gets too crowded and the takeout at night is very slow.                                                                                        1\n","Marisol at the front desk is serviceable; our sommelier, though not French, knew his Bordeaux, and our server was delightful.                                                           1\n","Too bad the food and service aren't nearly as nice as the decor.                                                                                                                        1\n","The wait staff didn't bother to refill my water until I had finish mine and was almost done with my husband's glass too.                                                                1\n","This pizza shop is one o fhte best places ever and is a hiden gem in a small community.                                                                                                 1\n","The complete and total rudeness and frustration surrounding this place started with the hostess calling us in the middle of the afternoon asking if we could change our reservation.    1\n","You have to be comfortable eating with your hands, sharing the same plate with your friends and be able to handle spicy food (not as spicy as some thai dishes though).                 1\n","Our waitress was pleasent and patient as we asked about all the various types of sushi.                                                                                                 1\n","since we're informed its service isslow we had dinner in midtownwent for dessertb-day thing.                                                                                            1\n","Name: text, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"k45Zoj6iWMcq"},"source":["# First Descriptive Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"gQkriKTZKlcm"},"source":["## Aspects per sentence"]},{"cell_type":"code","metadata":{"id":"fNv56kkP3OKP","executionInfo":{"status":"ok","timestamp":1624894590630,"user_tz":-120,"elapsed":15,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["pol_cols = [\"aspect_polarity_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ks804oRfuKxd","executionInfo":{"status":"ok","timestamp":1624894591273,"user_tz":-120,"elapsed":656,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"6abad54f-55c3-4a4e-d85e-94bb9a4c57f1"},"source":["prev = 0\n","total_asp = 0\n","for no, col in enumerate(pol_cols):\n","\n","    if col != \"aspect_polarity_1\":\n","        print(\"sentences with exactly \", no, \"aspects:\", prev - sum(df[col].value_counts()))\n","        total_asp += no * (prev - sum(df[col].value_counts()))\n","\n","    if col == \"aspect_polarity_\"+str(aspect_number-1):\n","        print(\"sentences with exactly \", no+1, \"aspects:\", sum(df[col].value_counts()))\n","        total_asp += (no+1) * sum(df[col].value_counts())\n","\n","\n","    prev = sum(df[col].value_counts())\n","\n","print(\"total no of aspects: \", total_asp)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["sentences with exactly  1 aspects: 0\n","sentences with exactly  2 aspects: 285\n","sentences with exactly  3 aspects: 136\n","sentences with exactly  4 aspects: 55\n","sentences with exactly  5 aspects: 16\n","sentences with exactly  6 aspects: 5\n","sentences with exactly  7 aspects: 2\n","sentences with exactly  8 aspects: 0\n","sentences with exactly  9 aspects: 0\n","sentences with exactly  10 aspects: 1\n","total no of aspects:  1332\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h_WVE4pSNbd8"},"source":["## Sentiment Frequency"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xitO9zfJUBWD","executionInfo":{"status":"ok","timestamp":1624894591274,"user_tz":-120,"elapsed":23,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"8189916d-8c95-4b91-c5c0-e1980f7cddc8"},"source":["df_pol = df.loc[:,pol_cols]\n","df_pol_counts = df_pol.apply(pd.Series.value_counts)\n","df_pol_counts.sum(axis=1)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    325.0\n","neutral     604.0\n","positive    403.0\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIhdlntE0lpQ","executionInfo":{"status":"ok","timestamp":1624894591275,"user_tz":-120,"elapsed":21,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"bb610724-9499-468b-cfda-8f5982006dcd"},"source":["sum(df_pol_counts.sum(axis=1))"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1332.0"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"FK6Qt7kHWcwu"},"source":["## Sentences with more than one aspect"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zs8e8UnQrJ8","executionInfo":{"status":"ok","timestamp":1624894591275,"user_tz":-120,"elapsed":18,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"c09d02e0-8d1d-4f7d-e96f-19f4973edc65"},"source":["multi_counter = 0\n","for line in df.index:\n","    sentiment_list = []\n","    for col in pol_cols:\n","        if df.loc[line,col] != None:\n","            sentiment_list += [df.loc[line,col]]\n","    if len(set(sentiment_list)) > 1:\n","        multi_counter += 1\n","multi_counter"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"TA33P7z7pJ26"},"source":["# Remove \"conflict\""]},{"cell_type":"code","metadata":{"id":"PMYoki_owON1","executionInfo":{"status":"ok","timestamp":1624894591277,"user_tz":-120,"elapsed":17,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["for line in range(len(df)):\n","    for col in pol_cols:\n","        if df.loc[line,col] == \"conflict\":\n","            df.loc[line,col] = None\n","            number = col[-1:]\n","            df.loc[line,\"aspect_term_\"+str(number)] = None\n","            df.loc[line,\"aspect_to_\"+str(number)] = None\n","            df.loc[line,\"aspect_from_\"+str(number)] = None"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"O7yBbHY4zFNV","executionInfo":{"status":"ok","timestamp":1624894591278,"user_tz":-120,"elapsed":17,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"7270ce22-4eb6-40e2-88ab-7ee1e3e55dca"},"source":["df"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>aspect_term_1</th>\n","      <th>aspect_polarity_1</th>\n","      <th>aspect_from_1</th>\n","      <th>aspect_to_1</th>\n","      <th>aspect_term_2</th>\n","      <th>aspect_polarity_2</th>\n","      <th>aspect_from_2</th>\n","      <th>aspect_to_2</th>\n","      <th>aspect_term_3</th>\n","      <th>aspect_polarity_3</th>\n","      <th>aspect_from_3</th>\n","      <th>aspect_to_3</th>\n","      <th>aspect_term_4</th>\n","      <th>aspect_polarity_4</th>\n","      <th>aspect_from_4</th>\n","      <th>aspect_to_4</th>\n","      <th>aspect_term_5</th>\n","      <th>aspect_polarity_5</th>\n","      <th>aspect_from_5</th>\n","      <th>aspect_to_5</th>\n","      <th>aspect_term_6</th>\n","      <th>aspect_polarity_6</th>\n","      <th>aspect_from_6</th>\n","      <th>aspect_to_6</th>\n","      <th>aspect_term_7</th>\n","      <th>aspect_polarity_7</th>\n","      <th>aspect_from_7</th>\n","      <th>aspect_to_7</th>\n","      <th>aspect_term_8</th>\n","      <th>aspect_polarity_8</th>\n","      <th>aspect_from_8</th>\n","      <th>aspect_to_8</th>\n","      <th>aspect_term_9</th>\n","      <th>aspect_polarity_9</th>\n","      <th>aspect_from_9</th>\n","      <th>aspect_to_9</th>\n","      <th>aspect_term_10</th>\n","      <th>aspect_polarity_10</th>\n","      <th>aspect_from_10</th>\n","      <th>aspect_to_10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>None</td>\n","      <td>After a couple of drinks, the apps--I like the...</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>18</td>\n","      <td>24</td>\n","      <td>roll</td>\n","      <td>positive</td>\n","      <td>68</td>\n","      <td>72</td>\n","      <td>cripsy squid</td>\n","      <td>positive</td>\n","      <td>81</td>\n","      <td>93</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>None</td>\n","      <td>The basil pepper mojito was a little daunting ...</td>\n","      <td>basil pepper mojito</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>23</td>\n","      <td>flavor</td>\n","      <td>positive</td>\n","      <td>85</td>\n","      <td>91</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>None</td>\n","      <td>Had to constantly ask the waiter to top up wat...</td>\n","      <td>waiter</td>\n","      <td>negative</td>\n","      <td>26</td>\n","      <td>32</td>\n","      <td>water glasses</td>\n","      <td>neutral</td>\n","      <td>43</td>\n","      <td>56</td>\n","      <td>service</td>\n","      <td>positive</td>\n","      <td>72</td>\n","      <td>79</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>None</td>\n","      <td>The portions were so small that we still wante...</td>\n","      <td>portions</td>\n","      <td>negative</td>\n","      <td>4</td>\n","      <td>12</td>\n","      <td>dinner</td>\n","      <td>neutral</td>\n","      <td>61</td>\n","      <td>67</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>None</td>\n","      <td>The staff is very kind and well trained, they'...</td>\n","      <td>staff</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>97</td>\n","      <td>100</td>\n","      <td>drinks</td>\n","      <td>neutral</td>\n","      <td>109</td>\n","      <td>115</td>\n","      <td>menu</td>\n","      <td>neutral</td>\n","      <td>156</td>\n","      <td>160</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>None</td>\n","      <td>Spinache rolls have lots of garlic.</td>\n","      <td>rolls</td>\n","      <td>neutral</td>\n","      <td>9</td>\n","      <td>14</td>\n","      <td>garlic</td>\n","      <td>positive</td>\n","      <td>28</td>\n","      <td>34</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>None</td>\n","      <td>I use to always get the turkey club, until rec...</td>\n","      <td>turkey club</td>\n","      <td>neutral</td>\n","      <td>24</td>\n","      <td>35</td>\n","      <td>bread</td>\n","      <td>positive</td>\n","      <td>77</td>\n","      <td>82</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>None</td>\n","      <td>The decor is worth a mention, with plush seati...</td>\n","      <td>decor</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>71</td>\n","      <td>74</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>None</td>\n","      <td>The Food The menu is better suited to the snac...</td>\n","      <td>Food</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>snacking</td>\n","      <td>positive</td>\n","      <td>42</td>\n","      <td>50</td>\n","      <td>bar</td>\n","      <td>neutral</td>\n","      <td>63</td>\n","      <td>66</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>None</td>\n","      <td>Another favorite is the hanger steak which is ...</td>\n","      <td>hanger steak</td>\n","      <td>positive</td>\n","      <td>24</td>\n","      <td>36</td>\n","      <td>amount of arugula</td>\n","      <td>positive</td>\n","      <td>101</td>\n","      <td>118</td>\n","      <td>a balsamic vinegar reduction served</td>\n","      <td>neutral</td>\n","      <td>123</td>\n","      <td>158</td>\n","      <td>sauce</td>\n","      <td>neutral</td>\n","      <td>164</td>\n","      <td>169</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 42 columns</p>\n","</div>"],"text/plain":["       id  ... aspect_to_10\n","0    None  ...         None\n","1    None  ...         None\n","2    None  ...         None\n","3    None  ...         None\n","4    None  ...         None\n","..    ...  ...          ...\n","495  None  ...         None\n","496  None  ...         None\n","497  None  ...         None\n","498  None  ...         None\n","499  None  ...         None\n","\n","[500 rows x 42 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"Yhf5gFid8__f"},"source":["# Check for wrong positions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nckKUb_U9BuI","executionInfo":{"status":"ok","timestamp":1624894591279,"user_tz":-120,"elapsed":16,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"8356dd47-a7e9-4a11-ec9d-9970b6514711"},"source":["mistakes = []\n","for ii in range(len(df)):\n","    for xx in range(1,aspect_number):\n","        asp_col = \"aspect_term_\"+str(xx)\n","        from_col = \"aspect_from_\"+str(xx)\n","        to_col = \"aspect_to_\"+str(xx)\n","        actual_term = df.loc[ii,asp_col]\n","        if actual_term != None:\n","            pos_term = df.text[ii][int(df.loc[ii,from_col]):int(df.loc[ii,to_col])]\n","            if actual_term != pos_term:\n","                mistakes += [ii]\n","                print(actual_term, pos_term)\n","mistakes"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"z8m9Y4Idr9Sa"},"source":["# Check for wrong aspect terms"]},{"cell_type":"code","metadata":{"id":"EB2miqWZnx_b","executionInfo":{"status":"ok","timestamp":1624894592378,"user_tz":-120,"elapsed":1112,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["import nltk"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OeQ1uqko757","executionInfo":{"status":"ok","timestamp":1624894592736,"user_tz":-120,"elapsed":364,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"f0db09a4-f3a9-41ba-eaf7-4122eff84c46"},"source":["nltk.download('punkt')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"LiykKcbXnYXm","executionInfo":{"status":"ok","timestamp":1624894592954,"user_tz":-120,"elapsed":225,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["for ii in range(len(df)):\n","    tokens = nltk.word_tokenize(df.text[ii])\n","    for xx in range(1,aspect_number):\n","        actual_term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","        if actual_term != None:\n","            for asp_part in nltk.word_tokenize(actual_term):\n","                if asp_part not in tokens and asp_part+\"-\" not in tokens:\n","                    print(ii,\"-\",xx,\":\",tokens)\n","                    print(actual_term, asp_part)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHbz2z97Qws1"},"source":["# Final Descriptive Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"Vt8ae9REQws3"},"source":["## Aspects per sentence"]},{"cell_type":"code","metadata":{"id":"bYbhl8WiQws4","executionInfo":{"status":"ok","timestamp":1624894593192,"user_tz":-120,"elapsed":243,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}}},"source":["pol_cols = [\"aspect_polarity_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8RwgLPzQws6","executionInfo":{"status":"ok","timestamp":1624894593193,"user_tz":-120,"elapsed":21,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"7b7e9cd0-8f46-46cc-c705-4a199b9ae20c"},"source":["prev = 0\n","total_asp = 0\n","for no, col in enumerate(pol_cols):\n","\n","    if col != \"aspect_polarity_1\":\n","        print(\"sentences with exactly \", no, \"aspects:\", prev - sum(df[col].value_counts()))\n","        total_asp += no * (prev - sum(df[col].value_counts()))\n","\n","    if col == \"aspect_polarity_\"+str(aspect_number-1):\n","        print(\"sentences with exactly \", no+1, \"aspects:\", sum(df[col].value_counts()))\n","        total_asp += (no+1) * sum(df[col].value_counts())\n","\n","\n","    prev = sum(df[col].value_counts())\n","\n","print(\"total no of aspects: \", total_asp)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["sentences with exactly  1 aspects: 0\n","sentences with exactly  2 aspects: 285\n","sentences with exactly  3 aspects: 136\n","sentences with exactly  4 aspects: 55\n","sentences with exactly  5 aspects: 16\n","sentences with exactly  6 aspects: 5\n","sentences with exactly  7 aspects: 2\n","sentences with exactly  8 aspects: 0\n","sentences with exactly  9 aspects: 0\n","sentences with exactly  10 aspects: 1\n","total no of aspects:  1332\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Nd69HsQQws8"},"source":["## Sentiment Frequency"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNL6i1u7Qws9","executionInfo":{"status":"ok","timestamp":1624894593193,"user_tz":-120,"elapsed":17,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"640dbe90-4996-46ba-bf83-3bc366255a95"},"source":["df_pol = df.loc[:,pol_cols]\n","df_pol_counts = df_pol.apply(pd.Series.value_counts)\n","df_pol_counts.sum(axis=1)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    325.0\n","neutral     604.0\n","positive    403.0\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29WedGDRQws_","executionInfo":{"status":"ok","timestamp":1624894593193,"user_tz":-120,"elapsed":12,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"1ffec967-6992-4be2-ca6d-d5485714a26d"},"source":["sum(df_pol_counts.sum(axis=1))"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1332.0"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"c_ZJhJIwQwtB"},"source":["## Sentences with more than one aspect"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1NLPh_AQwtD","executionInfo":{"status":"ok","timestamp":1624894593195,"user_tz":-120,"elapsed":10,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"cd88238b-0f9a-4da0-ef81-9b3a4100f2ac"},"source":["multi_counter = 0\n","for line in df.index:\n","    sentiment_list = []\n","    for col in pol_cols:\n","        if df.loc[line,col] != None:\n","            sentiment_list += [df.loc[line,col]]\n","    if len(set(sentiment_list)) > 1:\n","        multi_counter += 1\n","multi_counter"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"cpCOtYGA0OQd"},"source":["# Save as xml"]},{"cell_type":"code","metadata":{"id":"iYzlSnxhzl0N"},"source":["def xml_maker(df,aspect_number):\n","\n","    root = ET.Element('sentences')\n","\n","    for line in df.index:\n","        name = \"sentence\"\n","        entry = ET.SubElement(root, name)\n","        entry.set(\"id\", str(df[\"id\"][line]))\n","\n","        text_child = ET.SubElement(entry, \"text\")\n","        text_child.text = str(df[\"text\"][line])\n","\n","        asp_child = ET.SubElement(entry, \"aspectTerms\")\n","        for xx in range(1,aspect_number):\n","            if df.loc[line,\"aspect_term_\"+str(xx)] != None:\n","                asp_subchild = ET.SubElement(asp_child, \"aspectTerm\")\n","                asp_subchild.set(\"from\",str(df[\"aspect_from_\"+str(xx)][line]))\n","                asp_subchild.set(\"polarity\",str(df[\"aspect_polarity_\"+str(xx)][line]))\n","                asp_subchild.set(\"term\",str(df[\"aspect_term_\"+str(xx)][line]))\n","                asp_subchild.set(\"to\",str(df[\"aspect_to_\"+str(xx)][line]))\n","\n","    return ET.tostring(root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZ-FU7HtPq1H"},"source":["xml_data = xml_maker(df,aspect_number)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/val.xml\",\"w\") as f:\n","    f.write(xml_data.decode('utf-8'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iu9troUx3voZ"},"source":["# Create xml.seg"]},{"cell_type":"code","metadata":{"id":"8IvrB0Lf52zp"},"source":["def pol_to_no(sentiment):\n","  \n","    if sentiment == \"positive\":\n","        pol = 1\n","    elif sentiment == \"negative\":\n","        pol = -1\n","    elif sentiment == \"neutral\":\n","        pol = 0\n","\n","    return str(pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIXzzTPlRkwc"},"source":["def xml_seg_maker(df):\n","\n","    df_wo_id = df.drop(columns=\"id\", axis=1, inplace=False)\n","    df_wo_id.reset_index(drop=True, inplace=True)\n","    data_lines = []\n","\n","    for ii in df_wo_id.index:\n","\n","        line = list(df_wo_id.loc[ii])\n","        o_text = line[0]\n","        aspects = [line[xx] for xx in range(1,len(df_wo_id.loc[0]),4) if line[xx] != None]\n","        pols = [line[xx] for xx in range(2,len(df_wo_id.loc[0]),4) if line[xx] != None]\n","\n","        for asp in range(len(aspects)):\n","            text = o_text.replace(aspects[asp],'$T$')\n","            pol = pol_to_no(pols[asp])\n","\n","            data_lines += [text,aspects[asp],pol]\n","    \n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ND8pnKtwCLZL"},"source":["xml_seg_data = xml_seg_maker(df)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/val.xml.seg\",\"w\") as f:\n","    f.write('\\n'.join(xml_seg_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zY4NaEVH1Dhi"},"source":["# Create BERT+txt"]},{"cell_type":"code","metadata":{"id":"7c_Zb3Lk1vUa"},"source":["asp_cols = [\"aspect_term_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kopkDBAmmkxV"},"source":["def sent_conv(sentiment):\n","\n","    if sentiment == \"positive\":\n","        return \"POS\"\n","    elif sentiment == \"negative\":\n","        return \"NEG\"\n","    elif sentiment == \"neutral\":\n","        return \"NEU\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8E6pSYzt08i"},"source":["def txt_maker(df):\n","\n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.loc[line,\"text\"]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = sent_conv(df.loc[line,pol_cols[col]])\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for tok in tokens:\n","            if tok in asp_sent_dict.keys():\n","                label += tok + \"=T-\" + asp_sent_dict[tok] + \" \"\n","            else:\n","                label += tok + \"=O \"\n","        label = label[:-1]\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok in asp_sent_dict.keys():\n","                    new_pol = asp_sent_dict[new_tok]\n","                    old_label = \" \".join([tokens[no+xx]+\"=O\" for xx in range(ii) if no+xx < len(tokens)])\n","                    new_label = \" \".join([tokens[no+xx]+\"=T-\"+new_pol for xx in range(ii) if no+xx < len(tokens)])\n","                    label = label.replace(old_label, new_label)\n","  \n","        data_lines += [text+\"####\"+label]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvTsu2IyupwT"},"source":["txt_data = txt_maker(df)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/val.txt\",\"w\") as f:\n","    f.write('\\n'.join(txt_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4i-GN5Q1KlZ"},"source":["# Create RGATjson"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCyhkwcKwClh","executionInfo":{"status":"ok","timestamp":1620644912793,"user_tz":-120,"elapsed":37648,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"63e95e29-4cf6-4600-cd61-bac59488037b"},"source":["!pip install stanza"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting stanza\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n","\r\u001b[K     |█▏                              | 10kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.8.1+cu101)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (56.1.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n","Installing collected packages: stanza\n","Successfully installed stanza-1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Le-5FSclw2rm"},"source":["import stanza"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hp6-9OkNwJ1e","executionInfo":{"status":"ok","timestamp":1620645003401,"user_tz":-120,"elapsed":127494,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"16cb5dc9-eb11-44ab-ace7-911a9aaf9e3d"},"source":["stanza.download('en')\n","nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 34.2MB/s]                    \n","2021-05-10 11:08:35 INFO: Downloading default packages for language: en (English)...\n","Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:18<00:00, 5.26MB/s]\n","2021-05-10 11:10:01 INFO: Finished downloading models and saved to /root/stanza_resources.\n","2021-05-10 11:10:01 INFO: Loading these models for language: en (English):\n","========================\n","| Processor | Package  |\n","------------------------\n","| tokenize  | combined |\n","| pos       | combined |\n","| lemma     | combined |\n","| depparse  | combined |\n","========================\n","\n","2021-05-10 11:10:01 INFO: Use device: cpu\n","2021-05-10 11:10:01 INFO: Loading: tokenize\n","2021-05-10 11:10:01 INFO: Loading: pos\n","2021-05-10 11:10:02 INFO: Loading: lemma\n","2021-05-10 11:10:02 INFO: Loading: depparse\n","2021-05-10 11:10:02 INFO: Done loading processors!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IBmBwPg_yWGU"},"source":["from more_itertools import locate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGo5KiewzkCo"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvGzY7ydRUxU"},"source":["## in case of creating for the first time"]},{"cell_type":"code","metadata":{"id":"ILMTmvCG-sY6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620642554817,"user_tz":-120,"elapsed":383898,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"bc0870ff-2c5f-46d1-badc-d4c9caaff0bc"},"source":["def json_make_pos(df):\n","   \n","    manual_pos = {}\n","\n","    for ii in df.index:\n","        \n","        new_dict = {}\n","        text = df.loc[ii,\"text\"]\n","        tokens = [token.text for sentence in nlp(text).sentences for token in sentence.tokens]\n","        \n","        new_dict[\"aspects\"] = []\n","        for xx in range(1,aspect_number):         \n","            if df.loc[ii,\"aspect_term_\"+str(xx)] != None:\n","                asp_dict = {}   \n","                term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","\n","                # construct aspect position on token level\n","                asp_toks = [token.text for sentence in nlp(term).sentences for token in sentence.tokens]\n","                asp_ind = [list(locate(tokens, lambda a: a == tok)) for tok in asp_toks]\n","\n","                # for aspects appearing only once in text, take the correct position\n","                # otherwise set to None\n","                if len(asp_ind[0]) == 1:\n","                    from_index = asp_ind[0][0]\n","                else:\n","                    from_index = None\n","                if len(asp_ind[-1]) == 1:\n","                    to_index = asp_ind[-1][0] \n","                else: \n","                    to_index = None\n","\n","                # if both start and end pos are unknown, \n","                # e.g. for single-word aspects, \n","                # take character positions for help\n","                if from_index == None and to_index == None and len(asp_ind[0]) != 0:\n","                    print(ii, \": \", text)\n","                    print(\"original term: \",term)\n","                    all_char_from = [i for i in range(len(text)) if text.startswith(asp_toks[0], i)]\n","                    print(\"all start chars: \",all_char_from)\n","                    corr_char_from = int(df.loc[ii, \"aspect_from_\"+str(xx)])\n","                    print(\"correct start char: \",corr_char_from)\n","                    print(\"text beginning at correct start char: \", text[corr_char_from:])\n","                    print(\"tokens: \",tokens)\n","                    print(\"original asp tokens: \", asp_toks)\n","                    print(\"original asp indices: \",asp_ind)\n","                    if corr_char_from == max(all_char_from):\n","                        from_index = asp_ind[0][-1]\n","                    elif corr_char_from == min(all_char_from):\n","                        from_index = asp_ind[0][0]\n","\n","                # in case of missing start/end positions,\n","                # try to find \"to\"/\"from\" using aspect token number as distance\n","                if from_index == None and to_index != None:\n","                    from_index = to_index - len(asp_toks) +1\n","                if to_index == None and from_index != None:\n","                    to_index = from_index + len(asp_toks) -1\n","\n","                # correct tokenization errors in aspect term tokenization\n","                if from_index == None or to_index == None or asp_toks != tokens[from_index:to_index+1]:\n","\n","                    print(\"Tokenization Error in line \",ii,\"!\")\n","                    for pos, tok in enumerate(tokens):\n","                        print(pos, tok)\n","                    print(\"aspect term: \", term)\n","                    print(\"original asp tokens: \", asp_toks)\n","\n","                    from_index = int(input(\"start position?\"))\n","                    to_index = int(input(\"end position?\"))\n","\n","                    # add manually stated positions to dict for reproducibility\n","                    manual_pos[text] = {}\n","                    manual_pos[text][term] = {}\n","                    manual_pos[text][term][\"from\"] = from_index\n","                    manual_pos[text][term][\"to\"] = to_index\n","                    print(manual_pos)\n","\n","                    print(\"Final aspect tokens: \", tokens[from_index:to_index+1])\n","\n","                asp_dict[\"from\"] = from_index\n","                asp_dict[\"to\"] = to_index + 1\n","\n","    with open(\"/content/drive/My Drive/Masterarbeit/Data/preprocessing/aspect_positions/mams_val.json\",\"w\") as f:\n","        json.dump(manual_pos, f)\n","\n","json_make_pos(df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokenization Error in line  18 !\n","0 The\n","1 servers\n","2 ,\n","3 casual\n","4 in\n","5 their\n","6 striped\n","7 button\n","8 -downs\n","9 ,\n","10 anticipate\n","11 and\n","12 fulfill\n","13 needs\n","14 as\n","15 if\n","16 they\n","17 were\n","18 trained\n","19 as\n","20 mind\n","21 readers\n","22 .\n","aspect term:  striped button-downs\n","original asp tokens:  ['striped', 'button-downs']\n","start position?6\n","end position?8\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}}\n","Final aspect tokens:  ['striped', 'button', '-downs']\n","Tokenization Error in line  141 !\n","0 Other\n","1 small\n","2 dishes\n","3 ,\n","4 such\n","5 as\n","6 the\n","7 fragrant\n","8 sesame\n","9 oil\n","10 -\n","11 scented\n","12 raw\n","13 spicy\n","14 tuna\n","15 ,\n","16 are\n","17 excellent\n","18 with\n","19 a\n","20 beer-sized\n","21 mug\n","22 of\n","23 cold\n","24 oolong\n","25 tea\n","26 .\n","aspect term:  fragrant sesame oil-scented raw spicy tuna\n","original asp tokens:  ['fragrant', 'sesame', 'oil', '-scented', 'raw', 'spicy', 'tuna']\n","start position?7\n","end position?14\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}, 'Other small dishes, such as the fragrant sesame oil-scented raw spicy tuna, are excellent with a beer-sized mug of cold oolong tea.': {'fragrant sesame oil-scented raw spicy tuna': {'from': 7, 'to': 14}}}\n","Final aspect tokens:  ['fragrant', 'sesame', 'oil', '-', 'scented', 'raw', 'spicy', 'tuna']\n","Tokenization Error in line  163 !\n","0 Everything\n","1 on\n","2 the\n","3 menu\n","4 is\n","5 satisfyingly\n","6 savory\n","7 ,\n","8 from\n","9 a\n","10 simple\n","11 pot\n","12 of\n","13 mussels\n","14 in\n","15 a\n","16 choice\n","17 of\n","18 sauces\n","19 (\n","20 beer-and\n","21 -bacon\n","22 ,\n","23 creamy\n","24 mushroom\n","25 ,\n","26 or\n","27 white\n","28 -\n","29 wine\n","30 -\n","31 and\n","32 -\n","33 garlic\n","34 broth\n","35 )\n","36 ,\n","37 to\n","38 beef\n","39 stewed\n","40 with\n","41 beer\n","42 and\n","43 prunes\n","44 ,\n","45 to\n","46 a\n","47 juicy\n","48 croque-\n","49 monsieur\n","50 ,\n","51 and\n","52 beyond\n","53 .\n","aspect term:  white-wine-and-garlic broth\n","original asp tokens:  ['white-wine-and-garlic', 'broth']\n","start position?27\n","end position?34\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}, 'Other small dishes, such as the fragrant sesame oil-scented raw spicy tuna, are excellent with a beer-sized mug of cold oolong tea.': {'fragrant sesame oil-scented raw spicy tuna': {'from': 7, 'to': 14}}, 'Everything on the menu is satisfyingly savory, from a simple pot of mussels in a choice of sauces (beer-and-bacon, creamy mushroom, or white-wine-and-garlic broth), to beef stewed with beer and prunes, to a juicy croque-monsieur, and beyond.': {'white-wine-and-garlic broth': {'from': 27, 'to': 34}}}\n","Final aspect tokens:  ['white', '-', 'wine', '-', 'and', '-', 'garlic', 'broth']\n","Tokenization Error in line  205 !\n","0 Highlights\n","1 include\n","2 a\n","3 breaded\n","4 veal\n","5 chop\n","6 with\n","7 tomatoes\n","8 and\n","9 arugula\n","10 ,\n","11 a\n","12 pea-and\n","13 -\n","14 mushroom\n","15 risotto\n","16 topped\n","17 with\n","18 slivers\n","19 of\n","20 fried\n","21 zucchini\n","22 and\n","23 doused\n","24 with\n","25 truffle\n","26 oil\n","27 ,\n","28 and\n","29 a\n","30 top\n","31 -shelf\n","32 roasted\n","33 chicken\n","34 with\n","35 rapini\n","36 and\n","37 sausage\n","38 .\n","aspect term:  pea-and-mushroom risotto topped\n","original asp tokens:  ['pea-and', '-mushroom', 'risotto', 'topped']\n","start position?12\n","end position?16\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}, 'Other small dishes, such as the fragrant sesame oil-scented raw spicy tuna, are excellent with a beer-sized mug of cold oolong tea.': {'fragrant sesame oil-scented raw spicy tuna': {'from': 7, 'to': 14}}, 'Everything on the menu is satisfyingly savory, from a simple pot of mussels in a choice of sauces (beer-and-bacon, creamy mushroom, or white-wine-and-garlic broth), to beef stewed with beer and prunes, to a juicy croque-monsieur, and beyond.': {'white-wine-and-garlic broth': {'from': 27, 'to': 34}}, 'Highlights include a breaded veal chop with tomatoes and arugula, a pea-and-mushroom risotto topped with slivers of fried zucchini and doused with truffle oil, and a top-shelf roasted chicken with rapini and sausage.': {'pea-and-mushroom risotto topped': {'from': 12, 'to': 16}}}\n","Final aspect tokens:  ['pea-and', '-', 'mushroom', 'risotto', 'topped']\n","Tokenization Error in line  339 !\n","0 Dessert\n","1 :\n","2 Creme\n","3 Brulee\n","4 -\n","5 it\n","6 was\n","7 okay\n","8 ,\n","9 it\n","10 started\n","11 off\n","12 warm\n","13 on\n","14 the\n","15 edges\n","16 then\n","17 it\n","18 got\n","19 really\n","20 cold\n","21 in\n","22 the\n","23 middle\n","24 .\n","aspect term:  Creme Brulee-it\n","original asp tokens:  ['Creme', 'Brulee', '-it']\n","start position?2\n","end position?5\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}, 'Other small dishes, such as the fragrant sesame oil-scented raw spicy tuna, are excellent with a beer-sized mug of cold oolong tea.': {'fragrant sesame oil-scented raw spicy tuna': {'from': 7, 'to': 14}}, 'Everything on the menu is satisfyingly savory, from a simple pot of mussels in a choice of sauces (beer-and-bacon, creamy mushroom, or white-wine-and-garlic broth), to beef stewed with beer and prunes, to a juicy croque-monsieur, and beyond.': {'white-wine-and-garlic broth': {'from': 27, 'to': 34}}, 'Highlights include a breaded veal chop with tomatoes and arugula, a pea-and-mushroom risotto topped with slivers of fried zucchini and doused with truffle oil, and a top-shelf roasted chicken with rapini and sausage.': {'pea-and-mushroom risotto topped': {'from': 12, 'to': 16}}, 'Dessert: Creme Brulee-it was okay, it started off warm on the edges then it got really cold in the middle.': {'Creme Brulee-it': {'from': 2, 'to': 5}}}\n","Final aspect tokens:  ['Creme', 'Brulee', '-', 'it']\n","499 :  Another favorite is the hanger steak which is always cooked to perfection and served with a generous amount of arugula and a balsamic vinegar reduction served as a sauce.\n","original term:  a balsamic vinegar reduction served\n","all start chars:  [9, 25, 34, 46, 49, 74, 90, 101, 111, 117, 119, 123, 126, 129, 139, 159, 162, 165]\n","correct start char:  123\n","text beginning at correct start char:  a balsamic vinegar reduction served as a sauce.\n","tokens:  ['Another', 'favorite', 'is', 'the', 'hanger', 'steak', 'which', 'is', 'always', 'cooked', 'to', 'perfection', 'and', 'served', 'with', 'a', 'generous', 'amount', 'of', 'arugula', 'and', 'a', 'balsamic', 'vinegar', 'reduction', 'served', 'as', 'a', 'sauce', '.']\n","original asp tokens:  ['a', 'balsamic', 'vinegar', 'reduction', 'served']\n","original asp indices:  [[15, 21, 27], [22], [23], [24], [13, 25]]\n","Tokenization Error in line  499 !\n","0 Another\n","1 favorite\n","2 is\n","3 the\n","4 hanger\n","5 steak\n","6 which\n","7 is\n","8 always\n","9 cooked\n","10 to\n","11 perfection\n","12 and\n","13 served\n","14 with\n","15 a\n","16 generous\n","17 amount\n","18 of\n","19 arugula\n","20 and\n","21 a\n","22 balsamic\n","23 vinegar\n","24 reduction\n","25 served\n","26 as\n","27 a\n","28 sauce\n","29 .\n","aspect term:  a balsamic vinegar reduction served\n","original asp tokens:  ['a', 'balsamic', 'vinegar', 'reduction', 'served']\n","start position?21\n","end position?25\n","{'The servers, casual in their striped button-downs, anticipate and fulfill needs as if they were trained as mind readers.': {'striped button-downs': {'from': 6, 'to': 8}}, 'Other small dishes, such as the fragrant sesame oil-scented raw spicy tuna, are excellent with a beer-sized mug of cold oolong tea.': {'fragrant sesame oil-scented raw spicy tuna': {'from': 7, 'to': 14}}, 'Everything on the menu is satisfyingly savory, from a simple pot of mussels in a choice of sauces (beer-and-bacon, creamy mushroom, or white-wine-and-garlic broth), to beef stewed with beer and prunes, to a juicy croque-monsieur, and beyond.': {'white-wine-and-garlic broth': {'from': 27, 'to': 34}}, 'Highlights include a breaded veal chop with tomatoes and arugula, a pea-and-mushroom risotto topped with slivers of fried zucchini and doused with truffle oil, and a top-shelf roasted chicken with rapini and sausage.': {'pea-and-mushroom risotto topped': {'from': 12, 'to': 16}}, 'Dessert: Creme Brulee-it was okay, it started off warm on the edges then it got really cold in the middle.': {'Creme Brulee-it': {'from': 2, 'to': 5}}, 'Another favorite is the hanger steak which is always cooked to perfection and served with a generous amount of arugula and a balsamic vinegar reduction served as a sauce.': {'a balsamic vinegar reduction served': {'from': 21, 'to': 25}}}\n","Final aspect tokens:  ['a', 'balsamic', 'vinegar', 'reduction', 'served']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A9j2SYYxWT1y"},"source":["## in case of reproducing the dataset"]},{"cell_type":"code","metadata":{"id":"LbLhvcZ_Yag5"},"source":["def json_maker(df, manual_pos):\n","   \n","    new_data = []\n","\n","    for ii in df.index:\n","        \n","        new_dict = {}\n","        text = df.loc[ii,\"text\"]\n","\n","        tokens = [token.text for sentence in nlp(text).sentences for token in sentence.tokens]\n","        new_dict[\"token\"] = tokens\n","\n","        new_dict[\"pos\"] = [word.xpos for sentence in nlp(text).sentences for word in sentence.words]\n","        new_dict[\"head\"] = [str(word.head) for sentence in nlp(text).sentences for word in sentence.words]\n","        new_dict[\"deprel\"] = [word.deprel for sentence in nlp(text).sentences for word in sentence.words]\n","        \n","        new_dict[\"aspects\"] = []\n","        for xx in range(1,aspect_number):         \n","            if df.loc[ii,\"aspect_term_\"+str(xx)] != None:\n","                asp_dict = {}   \n","                term = df.loc[ii,\"aspect_term_\"+str(xx)]\n","                asp_dict[\"term\"] = term\n","                asp_dict[\"polarity\"] = df.loc[ii,\"aspect_polarity_\"+str(xx)]\n","\n","                # construct aspect position on token level\n","                asp_toks = [token.text for sentence in nlp(term).sentences for token in sentence.tokens]\n","                asp_ind = [list(locate(tokens, lambda a: a == term)) for term in asp_toks]\n","\n","                # for aspects appearing only once in text, take the correct position\n","                # otherwise set to None\n","                if len(asp_ind[0]) == 1:\n","                    from_index = asp_ind[0][0]\n","                else:\n","                    from_index = None\n","                if len(asp_ind[-1]) == 1:\n","                    to_index = asp_ind[-1][0] \n","                else: \n","                    to_index = None\n","\n","                # if both start and end pos are unknown, \n","                # e.g. for single-word aspects, \n","                # take character positions for help\n","                if from_index == None and to_index == None and len(asp_ind[0]) != 0:\n","\n","                    all_char_from = [i for i in range(len(text)) if text.startswith(asp_toks[0], i)]\n","                    corr_char_from = int(df.loc[ii, \"aspect_from_\"+str(xx)])\n","\n","                    if corr_char_from == max(all_char_from):\n","                        from_index = asp_ind[0][-1]\n","                    elif corr_char_from == min(all_char_from):\n","                        from_index = asp_ind[0][0]\n","\n","                # in case of missing start/end positions,\n","                # try to find \"to\"/\"from\" using aspect token number as distance\n","                if from_index == None and to_index != None:\n","                    from_index = to_index - len(asp_toks) +1\n","                if to_index == None and from_index != None:\n","                    to_index = from_index + len(asp_toks) -1\n","\n","                # correct tokenization errors in aspect term tokenization\n","                if from_index == None or to_index == None or asp_toks != tokens[from_index:to_index+1]:\n","                    if text in manual_pos.keys() and term in manual_pos[text].keys():\n","                        from_index = manual_pos[text][term][\"from\"]\n","                        to_index = manual_pos[text][term][\"to\"]\n","\n","                asp_dict[\"from\"] = from_index\n","                asp_dict[\"to\"] = to_index + 1\n","\n","                new_dict[\"aspects\"] += [asp_dict]\n","        \n","        new_data += [new_dict]\n","\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IbpMtyJ5XVXi"},"source":["pos_obj = open(\"/content/drive/My Drive/Masterarbeit/Data/preprocessing/aspect_positions/mams_val.json\")\n","loaded_pos = json.load(pos_obj)\n","\n","json_data = json_maker(df, loaded_pos)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/val.json\",\"w\") as f:\n","    json.dump(json_data, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aeHXUSmyaRfA"},"source":["# Create LCF-ATEPCdat"]},{"cell_type":"code","metadata":{"id":"si0ea14Q4PVZ"},"source":["def pol_to_no_shifted(sentiment):\n","  \n","    if sentiment == \"positive\":\n","        pol = 2\n","    elif sentiment == \"negative\":\n","        pol = 0\n","    elif sentiment == \"neutral\":\n","        pol = 1\n","\n","    return str(pol)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z0s04TbaVXo"},"source":["def dat_maker(df):\n","    \n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.loc[line,\"text\"]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = pol_to_no_shifted(df.loc[line,pol_cols[col]])\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for tok in tokens:\n","            if tok in asp_sent_dict.keys():\n","                label += tok + \" B-ASP -1\\n\"\n","            else:\n","                label += tok + \" O -1\\n\"\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok not in tokens and new_tok in asp_sent_dict.keys():\n","                    label = label.replace(tokens[no]+\" O -1\",tokens[no]+\" B-ASP -1\")\n","                    for xx in range(1,ii):\n","                        label = label.replace(tokens[no+xx]+\" O -1\",tokens[no+xx]+\" I-ASP -1\")\n","\n","        # create duplicates of review in case of more than one aspect\n","        for key, val in asp_sent_dict.items():\n","            if key in tokens:\n","                new_label = label.replace(key+\" B-ASP -1\", key+\" B-ASP \"+val)\n","                data_lines += [new_label]\n","                data_lines += [\"\\n\"]\n","            else:\n","                for ii in range(2,max_asp_len+1):\n","                    for no,tok in enumerate(tokens):\n","                        new_tok = \" \".join(tokens[no:no+ii])\n","                        if new_tok == key:\n","                            new_label = label.replace(tokens[no]+\" B-ASP -1\",tokens[no]+\" B-ASP \"+val)\n","                            for xx in range(1,ii):\n","                                new_label = new_label.replace(tokens[no+xx]+\" I-ASP -1\",tokens[no+xx]+\" I-ASP \"+val)\n","\n","                            data_lines += [new_label]\n","                            data_lines += [\"\\n\"]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n87vqlsKa36r"},"source":["dat_data = dat_maker(df)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/val.dat\",\"w\") as f:\n","    f.write(''.join(dat_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zccA_AJz1ZW4"},"source":["# Create GRACEtxt"]},{"cell_type":"code","metadata":{"id":"Jht_ESNPhmOY"},"source":["asp_cols = [\"aspect_term_\"+str(ii) for ii in range(1,aspect_number)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORfnB8CS6qjN"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce4Aqwxh7VGF","executionInfo":{"status":"ok","timestamp":1621531534186,"user_tz":-120,"elapsed":12,"user":{"displayName":"Elisabeth Lebmeier","photoUrl":"","userId":"15954454745832712963"}},"outputId":"eff19452-79fa-40d4-8185-1251c7c14b9a"},"source":["nltk.download('averaged_perceptron_tagger')\n","nltk.download('conll2000')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"al0gF6NL1cdd"},"source":["Create chunks and pos tags. Source: https://towardsdatascience.com/chunking-in-nlp-decoded-b4a71b2b4e24\n"]},{"cell_type":"code","metadata":{"id":"z-KWOrnf_5hn"},"source":["from nltk.tag import UnigramTagger, BigramTagger\n","from nltk.chunk import ChunkParserI\n","from nltk.chunk.util import tree2conlltags, conlltags2tree\n","from nltk.corpus import conll2000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mL0s_64z7oaF"},"source":["def conll_tag_chunks(chunk_sents):\n","    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n","    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n","    \n","def combined_tagger(train_data, taggers, backoff=None):\n","    for tagger in taggers:\n","        backoff = tagger(train_data, backoff=backoff)\n","    return backoff\n","\n","class NGramTagChunker(ChunkParserI):\n","\n","    def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\n","        train_sent_tags=conll_tag_chunks(train_sentences)\n","        self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)\n","    \n","    def parse(self,tagged_sentence):\n","        if not tagged_sentence:\n","            return None\n","        pos_tags=[tag for word, tag in tagged_sentence]\n","        chunk_pos_tags=self.chunk_tagger.tag(pos_tags)\n","        chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\n","        wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]\n","        return conlltags2tree(wpc_tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XBO64Z0ANbJ"},"source":["data = conll2000.chunked_sents()\n","ntc = NGramTagChunker(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUp02s4XBOY3"},"source":["Convert tags into BIOES scheme. Source: https://gist.github.com/allanj/5ad206f7f4645c0269b68fb2065712f4"]},{"cell_type":"code","metadata":{"id":"ZEtfTq5dC5Za"},"source":["def iob_iobes(tags):\n","    \"\"\"\n","    IOB2 (BIO) -> IOBES\n","    \"\"\"\n","    new_tags = []\n","    for i, tag in enumerate(tags):\n","        if tag == 'O':\n","            new_tags.append(tag)\n","        elif tag.split('-')[0] == 'B':\n","            if i + 1 != len(tags) and \\\n","                    tags[i + 1].split('-')[0] == 'I':\n","                new_tags.append(tag)\n","            else:\n","                new_tags.append(tag.replace('B-', 'S-'))\n","        elif tag.split('-')[0] == 'I':\n","            if i + 1 < len(tags) and \\\n","                    tags[i + 1].split('-')[0] == 'I':\n","                new_tags.append(tag)\n","            else:\n","                new_tags.append(tag.replace('I-', 'E-'))\n","        else:\n","            raise Exception('Invalid IOB format!')\n","    return new_tags"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQAlwhpqRwWS"},"source":["def grace_txt_maker(df):\n","    \n","    data_lines = []\n","\n","    for line in df.index:\n","\n","        text = df.text[line]\n","        tokens = nltk.word_tokenize(text)\n","\n","        # correct tokens\n","        for no,tok in enumerate(tokens):\n","            if tok[-1:] == \"-\" and len(tok)>2:\n","                tokens[no] = tok[:-1]\n","            if tok[:1] == \"'\" and len(tok)>3:\n","                tokens[no] = tok[1:]\n","            if tok in [\"'\",\"(\",\")\"]:\n","                tokens.remove(tok)\n","\n","        # create pos tags\n","        pos_tags = nltk.pos_tag(tokens)\n","\n","        # create chunk/phrase tags\n","        full_tags = tree2conlltags(ntc.parse(pos_tags))\n","        chunks_list = [full_tags[ii][2] for ii in range(len(full_tags))]\n","        new_chunks = iob_iobes(chunks_list)\n","\n","        # create aspect-polarity dict\n","        asp_sent_dict = {}\n","        max_asp_len = 0\n","        for col in range(len(asp_cols)):\n","            aspect = df.loc[line,asp_cols[col]]\n","            if aspect != None:\n","                asp_sent_dict[aspect] = df.loc[line,pol_cols[col]].upper()\n","                if len(aspect.split()) > max_asp_len:\n","                    max_asp_len = len(aspect.split())\n","\n","        label = \"\"\n","        # check for one-word-aspects\n","        for pos,tok in enumerate(tokens):\n","            label += tok + \" \" + pos_tags[pos][1] + \" \" + new_chunks[pos]\n","            if tok in asp_sent_dict.keys():\n","                label +=  \" B_AP \" + asp_sent_dict[tok] + \" B_AP+\" + asp_sent_dict[tok] + \"\\n\"\n","            else:\n","                label += \" O O O \\n\"\n","\n","        # check for multi-word-aspects\n","        for ii in range(2,max_asp_len+1):\n","            for no,tok in enumerate(tokens):\n","                new_tok = \" \".join(tokens[no:no+ii])\n","                if new_tok not in tokens and new_tok in asp_sent_dict.keys():\n","                    new_pol = asp_sent_dict[new_tok]\n","                    label = label.replace(tokens[no]+ \" \" + pos_tags[no][1] + \" \" + new_chunks[no] + \" O O O \\n\",\n","                                          tokens[no]+ \" \" + pos_tags[no][1] + \" \" + new_chunks[no] + \" B_AP \" + \\\n","                                          new_pol + \" B_AP+\" + new_pol + \"\\n\")\n","                    for xx in range(1,ii):\n","                        label = label.replace(tokens[no+xx] + \" \" + pos_tags[no+xx][1] + \" \" + new_chunks[no+xx] + \" O O O \\n\",\n","                                          tokens[no+xx]+ \" \" + pos_tags[no+xx][1] + \" \" + new_chunks[no+xx] + \" I_AP \" + \\\n","                                          new_pol + \" I_AP+\" + new_pol + \"\\n\")\n","\n","        data_lines += [label]\n","\n","    return data_lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jCIhax6erbg"},"source":["grace_txt_data = grace_txt_maker(df)\n","\n","with open(\"/content/drive/My Drive/Masterarbeit/Data/Final/MAMS/grace_val.txt\",\"w\") as f:\n","    f.write('\\n'.join(grace_txt_data))"],"execution_count":null,"outputs":[]}]}